{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `compute_loss` function below:\n",
    "<a id='compute_loss'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    y = np.array([y]).T.reshape([len(y), 1])\n",
    "    w = np.array([w]).T.reshape([len(w), 1])\n",
    "    \n",
    "    e = y - np.matmul(tx,w) \n",
    "    \n",
    "    return (1/y.shape[0]) * (e.T.dot(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Grid Search\n",
    "\n",
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    \n",
    "    for i in range(0, w0.shape[0]):\n",
    "        for j in range(0, w1.shape[0]):\n",
    "            losses[i, j] = compute_loss_mse(y, tx, np.array([[w0[i]], [w1[j]]]))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100.          -99.3987976   -98.79759519  -98.19639279  -97.59519038\n",
      "  -96.99398798  -96.39278557  -95.79158317  -95.19038076  -94.58917836\n",
      "  -93.98797595  -93.38677355  -92.78557114  -92.18436874  -91.58316633\n",
      "  -90.98196393  -90.38076152  -89.77955912  -89.17835671  -88.57715431\n",
      "  -87.9759519   -87.3747495   -86.77354709  -86.17234469  -85.57114228\n",
      "  -84.96993988  -84.36873747  -83.76753507  -83.16633267  -82.56513026\n",
      "  -81.96392786  -81.36272545  -80.76152305  -80.16032064  -79.55911824\n",
      "  -78.95791583  -78.35671343  -77.75551102  -77.15430862  -76.55310621\n",
      "  -75.95190381  -75.3507014   -74.749499    -74.14829659  -73.54709419\n",
      "  -72.94589178  -72.34468938  -71.74348697  -71.14228457  -70.54108216\n",
      "  -69.93987976  -69.33867735  -68.73747495  -68.13627255  -67.53507014\n",
      "  -66.93386774  -66.33266533  -65.73146293  -65.13026052  -64.52905812\n",
      "  -63.92785571  -63.32665331  -62.7254509   -62.1242485   -61.52304609\n",
      "  -60.92184369  -60.32064128  -59.71943888  -59.11823647  -58.51703407\n",
      "  -57.91583166  -57.31462926  -56.71342685  -56.11222445  -55.51102204\n",
      "  -54.90981964  -54.30861723  -53.70741483  -53.10621242  -52.50501002\n",
      "  -51.90380762  -51.30260521  -50.70140281  -50.1002004   -49.498998\n",
      "  -48.89779559  -48.29659319  -47.69539078  -47.09418838  -46.49298597\n",
      "  -45.89178357  -45.29058116  -44.68937876  -44.08817635  -43.48697395\n",
      "  -42.88577154  -42.28456914  -41.68336673  -41.08216433  -40.48096192\n",
      "  -39.87975952  -39.27855711  -38.67735471  -38.0761523   -37.4749499\n",
      "  -36.87374749  -36.27254509  -35.67134269  -35.07014028  -34.46893788\n",
      "  -33.86773547  -33.26653307  -32.66533066  -32.06412826  -31.46292585\n",
      "  -30.86172345  -30.26052104  -29.65931864  -29.05811623  -28.45691383\n",
      "  -27.85571142  -27.25450902  -26.65330661  -26.05210421  -25.4509018\n",
      "  -24.8496994   -24.24849699  -23.64729459  -23.04609218  -22.44488978\n",
      "  -21.84368737  -21.24248497  -20.64128257  -20.04008016  -19.43887776\n",
      "  -18.83767535  -18.23647295  -17.63527054  -17.03406814  -16.43286573\n",
      "  -15.83166333  -15.23046092  -14.62925852  -14.02805611  -13.42685371\n",
      "  -12.8256513   -12.2244489   -11.62324649  -11.02204409  -10.42084168\n",
      "   -9.81963928   -9.21843687   -8.61723447   -8.01603206   -7.41482966\n",
      "   -6.81362725   -6.21242485   -5.61122244   -5.01002004   -4.40881764\n",
      "   -3.80761523   -3.20641283   -2.60521042   -2.00400802   -1.40280561\n",
      "   -0.80160321   -0.2004008     0.4008016     1.00200401    1.60320641\n",
      "    2.20440882    2.80561122    3.40681363    4.00801603    4.60921844\n",
      "    5.21042084    5.81162325    6.41282565    7.01402806    7.61523046\n",
      "    8.21643287    8.81763527    9.41883768   10.02004008   10.62124248\n",
      "   11.22244489   11.82364729   12.4248497    13.0260521    13.62725451\n",
      "   14.22845691   14.82965932   15.43086172   16.03206413   16.63326653\n",
      "   17.23446894   17.83567134   18.43687375   19.03807615   19.63927856\n",
      "   20.24048096   20.84168337   21.44288577   22.04408818   22.64529058\n",
      "   23.24649299   23.84769539   24.4488978    25.0501002    25.65130261\n",
      "   26.25250501   26.85370741   27.45490982   28.05611222   28.65731463\n",
      "   29.25851703   29.85971944   30.46092184   31.06212425   31.66332665\n",
      "   32.26452906   32.86573146   33.46693387   34.06813627   34.66933868\n",
      "   35.27054108   35.87174349   36.47294589   37.0741483    37.6753507\n",
      "   38.27655311   38.87775551   39.47895792   40.08016032   40.68136273\n",
      "   41.28256513   41.88376754   42.48496994   43.08617234   43.68737475\n",
      "   44.28857715   44.88977956   45.49098196   46.09218437   46.69338677\n",
      "   47.29458918   47.89579158   48.49699399   49.09819639   49.6993988\n",
      "   50.3006012    50.90180361   51.50300601   52.10420842   52.70541082\n",
      "   53.30661323   53.90781563   54.50901804   55.11022044   55.71142285\n",
      "   56.31262525   56.91382766   57.51503006   58.11623246   58.71743487\n",
      "   59.31863727   59.91983968   60.52104208   61.12224449   61.72344689\n",
      "   62.3246493    62.9258517    63.52705411   64.12825651   64.72945892\n",
      "   65.33066132   65.93186373   66.53306613   67.13426854   67.73547094\n",
      "   68.33667335   68.93787575   69.53907816   70.14028056   70.74148297\n",
      "   71.34268537   71.94388778   72.54509018   73.14629259   73.74749499\n",
      "   74.34869739   74.9498998    75.5511022    76.15230461   76.75350701\n",
      "   77.35470942   77.95591182   78.55711423   79.15831663   79.75951904\n",
      "   80.36072144   80.96192385   81.56312625   82.16432866   82.76553106\n",
      "   83.36673347   83.96793587   84.56913828   85.17034068   85.77154309\n",
      "   86.37274549   86.9739479    87.5751503    88.17635271   88.77755511\n",
      "   89.37875752   89.97995992   90.58116232   91.18236473   91.78356713\n",
      "   92.38476954   92.98597194   93.58717435   94.18837675   94.78957916\n",
      "   95.39078156   95.99198397   96.59318637   97.19438878   97.79559118\n",
      "   98.39679359   98.99799599   99.5991984   100.2004008   100.80160321\n",
      "  101.40280561  102.00400802  102.60521042  103.20641283  103.80761523\n",
      "  104.40881764  105.01002004  105.61122244  106.21242485  106.81362725\n",
      "  107.41482966  108.01603206  108.61723447  109.21843687  109.81963928\n",
      "  110.42084168  111.02204409  111.62324649  112.2244489   112.8256513\n",
      "  113.42685371  114.02805611  114.62925852  115.23046092  115.83166333\n",
      "  116.43286573  117.03406814  117.63527054  118.23647295  118.83767535\n",
      "  119.43887776  120.04008016  120.64128257  121.24248497  121.84368737\n",
      "  122.44488978  123.04609218  123.64729459  124.24849699  124.8496994\n",
      "  125.4509018   126.05210421  126.65330661  127.25450902  127.85571142\n",
      "  128.45691383  129.05811623  129.65931864  130.26052104  130.86172345\n",
      "  131.46292585  132.06412826  132.66533066  133.26653307  133.86773547\n",
      "  134.46893788  135.07014028  135.67134269  136.27254509  136.87374749\n",
      "  137.4749499   138.0761523   138.67735471  139.27855711  139.87975952\n",
      "  140.48096192  141.08216433  141.68336673  142.28456914  142.88577154\n",
      "  143.48697395  144.08817635  144.68937876  145.29058116  145.89178357\n",
      "  146.49298597  147.09418838  147.69539078  148.29659319  148.89779559\n",
      "  149.498998    150.1002004   150.70140281  151.30260521  151.90380762\n",
      "  152.50501002  153.10621242  153.70741483  154.30861723  154.90981964\n",
      "  155.51102204  156.11222445  156.71342685  157.31462926  157.91583166\n",
      "  158.51703407  159.11823647  159.71943888  160.32064128  160.92184369\n",
      "  161.52304609  162.1242485   162.7254509   163.32665331  163.92785571\n",
      "  164.52905812  165.13026052  165.73146293  166.33266533  166.93386774\n",
      "  167.53507014  168.13627255  168.73747495  169.33867735  169.93987976\n",
      "  170.54108216  171.14228457  171.74348697  172.34468938  172.94589178\n",
      "  173.54709419  174.14829659  174.749499    175.3507014   175.95190381\n",
      "  176.55310621  177.15430862  177.75551102  178.35671343  178.95791583\n",
      "  179.55911824  180.16032064  180.76152305  181.36272545  181.96392786\n",
      "  182.56513026  183.16633267  183.76753507  184.36873747  184.96993988\n",
      "  185.57114228  186.17234469  186.77354709  187.3747495   187.9759519\n",
      "  188.57715431  189.17835671  189.77955912  190.38076152  190.98196393\n",
      "  191.58316633  192.18436874  192.78557114  193.38677355  193.98797595\n",
      "  194.58917836  195.19038076  195.79158317  196.39278557  196.99398798\n",
      "  197.59519038  198.19639279  198.79759519  199.3987976   200.        ]\n",
      "Grid Search: loss*=30.79581141642729, w0*=73.14629258517036, w1*=13.527054108216447, execution time=10.225 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=500)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "\n",
    "    e = y - np.matmul(tx,w)\n",
    "    return (-1/y.shape[0]) * np.matmul(tx.T, e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mse(y, tx, w)\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=[[5584.47342552]], w0=36.646961001052595, w1=6.7398562174944665\n",
      "Gradient Descent(1/99): loss=[[1419.19718818]], w0=54.97044150157889, w1=10.10978432624174\n",
      "Gradient Descent(2/99): loss=[[377.87812885]], w0=64.13218175184204, w1=11.794748380615387\n",
      "Gradient Descent(3/99): loss=[[117.54836402]], w0=68.71305187697362, w1=12.637230407802214\n",
      "Gradient Descent(4/99): loss=[[52.46592281]], w0=71.0034869395394, w1=13.058471421395629\n",
      "Gradient Descent(5/99): loss=[[36.19531251]], w0=72.1487044708223, w1=13.269091928192337\n",
      "Gradient Descent(6/99): loss=[[32.12765993]], w0=72.72131323646374, w1=13.374402181590693\n",
      "Gradient Descent(7/99): loss=[[31.11074679]], w0=73.00761761928446, w1=13.42705730828987\n",
      "Gradient Descent(8/99): loss=[[30.8565185]], w0=73.15076981069483, w1=13.453384871639459\n",
      "Gradient Descent(9/99): loss=[[30.79296143]], w0=73.22234590640001, w1=13.466548653314254\n",
      "Gradient Descent(10/99): loss=[[30.77707216]], w0=73.2581339542526, w1=13.473130544151651\n",
      "Gradient Descent(11/99): loss=[[30.77309984]], w0=73.27602797817889, w1=13.47642148957035\n",
      "Gradient Descent(12/99): loss=[[30.77210676]], w0=73.28497499014203, w1=13.4780669622797\n",
      "Gradient Descent(13/99): loss=[[30.77185849]], w0=73.28944849612361, w1=13.478889698634374\n",
      "Gradient Descent(14/99): loss=[[30.77179643]], w0=73.2916852491144, w1=13.479301066811711\n",
      "Gradient Descent(15/99): loss=[[30.77178091]], w0=73.29280362560979, w1=13.47950675090038\n",
      "Gradient Descent(16/99): loss=[[30.77177703]], w0=73.29336281385748, w1=13.479609592944716\n",
      "Gradient Descent(17/99): loss=[[30.77177606]], w0=73.29364240798134, w1=13.479661013966883\n",
      "Gradient Descent(18/99): loss=[[30.77177582]], w0=73.29378220504326, w1=13.479686724477967\n",
      "Gradient Descent(19/99): loss=[[30.77177576]], w0=73.29385210357422, w1=13.479699579733508\n",
      "Gradient Descent(20/99): loss=[[30.77177574]], w0=73.29388705283971, w1=13.479706007361278\n",
      "Gradient Descent(21/99): loss=[[30.77177574]], w0=73.29390452747245, w1=13.479709221175163\n",
      "Gradient Descent(22/99): loss=[[30.77177574]], w0=73.29391326478881, w1=13.479710828082107\n",
      "Gradient Descent(23/99): loss=[[30.77177574]], w0=73.293917633447, w1=13.479711631535578\n",
      "Gradient Descent(24/99): loss=[[30.77177574]], w0=73.2939198177761, w1=13.479712033262313\n",
      "Gradient Descent(25/99): loss=[[30.77177574]], w0=73.29392090994064, w1=13.479712234125682\n",
      "Gradient Descent(26/99): loss=[[30.77177574]], w0=73.29392145602291, w1=13.479712334557366\n",
      "Gradient Descent(27/99): loss=[[30.77177574]], w0=73.29392172906405, w1=13.479712384773208\n",
      "Gradient Descent(28/99): loss=[[30.77177574]], w0=73.29392186558462, w1=13.47971240988113\n",
      "Gradient Descent(29/99): loss=[[30.77177574]], w0=73.2939219338449, w1=13.479712422435089\n",
      "Gradient Descent(30/99): loss=[[30.77177574]], w0=73.29392196797504, w1=13.47971242871207\n",
      "Gradient Descent(31/99): loss=[[30.77177574]], w0=73.29392198504011, w1=13.47971243185056\n",
      "Gradient Descent(32/99): loss=[[30.77177574]], w0=73.29392199357265, w1=13.479712433419804\n",
      "Gradient Descent(33/99): loss=[[30.77177574]], w0=73.29392199783892, w1=13.479712434204426\n",
      "Gradient Descent(34/99): loss=[[30.77177574]], w0=73.29392199997206, w1=13.479712434596738\n",
      "Gradient Descent(35/99): loss=[[30.77177574]], w0=73.29392200103862, w1=13.479712434792894\n",
      "Gradient Descent(36/99): loss=[[30.77177574]], w0=73.2939220015719, w1=13.479712434890972\n",
      "Gradient Descent(37/99): loss=[[30.77177574]], w0=73.29392200183854, w1=13.47971243494001\n",
      "Gradient Descent(38/99): loss=[[30.77177574]], w0=73.29392200197186, w1=13.47971243496453\n",
      "Gradient Descent(39/99): loss=[[30.77177574]], w0=73.29392200203853, w1=13.47971243497679\n",
      "Gradient Descent(40/99): loss=[[30.77177574]], w0=73.29392200207185, w1=13.47971243498292\n",
      "Gradient Descent(41/99): loss=[[30.77177574]], w0=73.29392200208852, w1=13.479712434985984\n",
      "Gradient Descent(42/99): loss=[[30.77177574]], w0=73.29392200209685, w1=13.479712434987517\n",
      "Gradient Descent(43/99): loss=[[30.77177574]], w0=73.29392200210101, w1=13.479712434988283\n",
      "Gradient Descent(44/99): loss=[[30.77177574]], w0=73.2939220021031, w1=13.479712434988667\n",
      "Gradient Descent(45/99): loss=[[30.77177574]], w0=73.29392200210414, w1=13.479712434988858\n",
      "Gradient Descent(46/99): loss=[[30.77177574]], w0=73.29392200210467, w1=13.479712434988954\n",
      "Gradient Descent(47/99): loss=[[30.77177574]], w0=73.29392200210492, w1=13.479712434989002\n",
      "Gradient Descent(48/99): loss=[[30.77177574]], w0=73.29392200210505, w1=13.479712434989025\n",
      "Gradient Descent(49/99): loss=[[30.77177574]], w0=73.29392200210512, w1=13.479712434989038\n",
      "Gradient Descent(50/99): loss=[[30.77177574]], w0=73.29392200210516, w1=13.479712434989043\n",
      "Gradient Descent(51/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989047\n",
      "Gradient Descent(52/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(53/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(54/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(55/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(56/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(57/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(58/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(59/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(60/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(61/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(62/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(63/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(64/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(65/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(66/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(67/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(68/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(69/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(70/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(71/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(72/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(73/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(74/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(75/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(76/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(77/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(78/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(79/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(80/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(81/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(82/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(83/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(84/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(85/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(86/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(87/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(88/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(89/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(90/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(91/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(92/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(93/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(94/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(95/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(96/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(97/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(98/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent(99/99): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989048\n",
      "Gradient Descent: execution time=0.057 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "\n",
    "gamma = 0.5\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e09bb9c8dfb416f90143e76dc60852b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=101, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    \n",
    "    return compute_gradient(y, tx, w)\n",
    "    \n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "      \n",
    "    initial_w = initial_w.reshape((2,1))\n",
    "    \n",
    "    gen = batch_iter(y, tx, batch_size)\n",
    "    \n",
    "    y = []\n",
    "    tx = []\n",
    "    for pair in gen:\n",
    "        y.append(pair[0])\n",
    "        tx.append(pair[1])\n",
    "        \n",
    "    y = np.asarray(y).T\n",
    "    tx = np.asarray(tx).reshape((100,2))\n",
    "    \n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mse(y, tx, [w[0], w[1]])\n",
    "        grad = compute_stoch_gradient(y, tx, [w[0], w[1]])\n",
    "\n",
    "        w = w - (gamma * grad)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=[[5738.18991451]], w0=[37.29974646], w1=[6.5490384]\n",
      "Gradient Descent(1/99): loss=[[1442.11766525]], w0=[55.88702092], w1=[9.6906462]\n",
      "Gradient Descent(2/99): loss=[[377.60687541]], w0=[65.15062919], w1=[11.19105447]\n",
      "Gradient Descent(3/99): loss=[[113.78756965]], w0=[69.76809172], w1=[11.90394417]\n",
      "Gradient Descent(4/99): loss=[[48.39120404]], w0=[72.07000885], w1=[12.24059479]\n",
      "Gradient Descent(5/99): loss=[[32.17662515]], w0=[73.21774955], w1=[12.39841223]\n",
      "Gradient Descent(6/99): loss=[[28.15520454]], w0=[73.79011141], w1=[12.47173899]\n",
      "Gradient Descent(7/99): loss=[[27.15751728]], w0=[74.07559145], w1=[12.50543521]\n",
      "Gradient Descent(8/99): loss=[[26.90990468]], w0=[74.21800938], w1=[12.52070513]\n",
      "Gradient Descent(9/99): loss=[[26.84842378]], w0=[74.28907239], w1=[12.52750018]\n",
      "Gradient Descent(10/99): loss=[[26.83315072]], w0=[74.32453895], w1=[12.53045047]\n",
      "Gradient Descent(11/99): loss=[[26.82935439]], w0=[74.34224402], w1=[12.53168735]\n",
      "Gradient Descent(12/99): loss=[[26.82841012]], w0=[74.35108474], w1=[12.53217879]\n",
      "Gradient Descent(13/99): loss=[[26.82817507]], w0=[74.3555004], w1=[12.53235678]\n",
      "Gradient Descent(14/99): loss=[[26.82811651]], w0=[74.35770653], w1=[12.53240965]\n",
      "Gradient Descent(15/99): loss=[[26.8281019]], w0=[74.35880909], w1=[12.5324168]\n",
      "Gradient Descent(16/99): loss=[[26.82809826]], w0=[74.3593603], w1=[12.53241008]\n",
      "Gradient Descent(17/99): loss=[[26.82809734]], w0=[74.35963597], w1=[12.53240123]\n",
      "Gradient Descent(18/99): loss=[[26.82809712]], w0=[74.35977389], w1=[12.53239386]\n",
      "Gradient Descent(19/99): loss=[[26.82809706]], w0=[74.35984292], w1=[12.53238861]\n",
      "Gradient Descent(20/99): loss=[[26.82809704]], w0=[74.35987748], w1=[12.53238514]\n",
      "Gradient Descent(21/99): loss=[[26.82809704]], w0=[74.3598948], w1=[12.53238296]\n",
      "Gradient Descent(22/99): loss=[[26.82809704]], w0=[74.35990348], w1=[12.53238163]\n",
      "Gradient Descent(23/99): loss=[[26.82809704]], w0=[74.35990783], w1=[12.53238084]\n",
      "Gradient Descent(24/99): loss=[[26.82809704]], w0=[74.35991001], w1=[12.53238037]\n",
      "Gradient Descent(25/99): loss=[[26.82809704]], w0=[74.35991111], w1=[12.5323801]\n",
      "Gradient Descent(26/99): loss=[[26.82809704]], w0=[74.35991166], w1=[12.53237995]\n",
      "Gradient Descent(27/99): loss=[[26.82809704]], w0=[74.35991194], w1=[12.53237986]\n",
      "Gradient Descent(28/99): loss=[[26.82809704]], w0=[74.35991208], w1=[12.53237981]\n",
      "Gradient Descent(29/99): loss=[[26.82809704]], w0=[74.35991215], w1=[12.53237978]\n",
      "Gradient Descent(30/99): loss=[[26.82809704]], w0=[74.35991218], w1=[12.53237977]\n",
      "Gradient Descent(31/99): loss=[[26.82809704]], w0=[74.3599122], w1=[12.53237976]\n",
      "Gradient Descent(32/99): loss=[[26.82809704]], w0=[74.35991221], w1=[12.53237975]\n",
      "Gradient Descent(33/99): loss=[[26.82809704]], w0=[74.35991221], w1=[12.53237975]\n",
      "Gradient Descent(34/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(35/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(36/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(37/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(38/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(39/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(40/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(41/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(42/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(43/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(44/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(45/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(46/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(47/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(48/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(49/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(50/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(51/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(52/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(53/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(54/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(55/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(56/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(57/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(58/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(59/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(60/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(61/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(62/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(63/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(64/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(65/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(66/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(67/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(68/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(69/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(70/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(71/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(72/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(73/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(74/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(75/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(76/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(77/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(78/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(79/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(80/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(81/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(82/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(83/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(84/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(85/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(86/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(87/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(88/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(89/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(90/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(91/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(92/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(93/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(94/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(95/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(96/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(97/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(98/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "Gradient Descent(99/99): loss=[[26.82809704]], w0=[74.35991222], w1=[12.53237975]\n",
      "SGD: execution time=0.051 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "batch_size = 100\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b22bfa9de34fb8aff6a32c0acfe355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=101, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Effect of Outliers and MAE Cost Function, and Subgradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Load and plot data containing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=[[5739.67022907]], w0=51.84746409844842, w1=7.7244264061924195\n",
      "Gradient Descent(1/149): loss=[[636.5642494]], w0=67.40170332798297, w1=10.041754328050114\n",
      "Gradient Descent(2/149): loss=[[177.28471123]], w0=72.06797509684336, w1=10.736952704607411\n",
      "Gradient Descent(3/149): loss=[[135.9495528]], w0=73.46785662750146, w1=10.945512217574597\n",
      "Gradient Descent(4/149): loss=[[132.22938854]], w0=73.88782108669889, w1=11.00808007146475\n",
      "Gradient Descent(5/149): loss=[[131.89457376]], w0=74.01381042445813, w1=11.026850427631798\n",
      "Gradient Descent(6/149): loss=[[131.86444042]], w0=74.0516072257859, w1=11.032481534481914\n",
      "Gradient Descent(7/149): loss=[[131.86172842]], w0=74.06294626618423, w1=11.034170866536945\n",
      "Gradient Descent(8/149): loss=[[131.86148434]], w0=74.06634797830372, w1=11.034677666153454\n",
      "Gradient Descent(9/149): loss=[[131.86146238]], w0=74.06736849193958, w1=11.034829706038408\n",
      "Gradient Descent(10/149): loss=[[131.8614604]], w0=74.06767464603033, w1=11.034875318003895\n",
      "Gradient Descent(11/149): loss=[[131.86146022]], w0=74.06776649225755, w1=11.034889001593541\n",
      "Gradient Descent(12/149): loss=[[131.86146021]], w0=74.06779404612573, w1=11.034893106670431\n",
      "Gradient Descent(13/149): loss=[[131.86146021]], w0=74.06780231228618, w1=11.034894338193501\n",
      "Gradient Descent(14/149): loss=[[131.86146021]], w0=74.06780479213431, w1=11.034894707650421\n",
      "Gradient Descent(15/149): loss=[[131.86146021]], w0=74.06780553608874, w1=11.034894818487496\n",
      "Gradient Descent(16/149): loss=[[131.86146021]], w0=74.06780575927507, w1=11.03489485173862\n",
      "Gradient Descent(17/149): loss=[[131.86146021]], w0=74.06780582623098, w1=11.034894861713957\n",
      "Gradient Descent(18/149): loss=[[131.86146021]], w0=74.06780584631775, w1=11.034894864706557\n",
      "Gradient Descent(19/149): loss=[[131.86146021]], w0=74.06780585234378, w1=11.034894865604338\n",
      "Gradient Descent(20/149): loss=[[131.86146021]], w0=74.06780585415159, w1=11.034894865873675\n",
      "Gradient Descent(21/149): loss=[[131.86146021]], w0=74.06780585469393, w1=11.034894865954474\n",
      "Gradient Descent(22/149): loss=[[131.86146021]], w0=74.06780585485663, w1=11.034894865978712\n",
      "Gradient Descent(23/149): loss=[[131.86146021]], w0=74.06780585490544, w1=11.034894865985985\n",
      "Gradient Descent(24/149): loss=[[131.86146021]], w0=74.0678058549201, w1=11.034894865988166\n",
      "Gradient Descent(25/149): loss=[[131.86146021]], w0=74.06780585492449, w1=11.034894865988822\n",
      "Gradient Descent(26/149): loss=[[131.86146021]], w0=74.06780585492581, w1=11.034894865989015\n",
      "Gradient Descent(27/149): loss=[[131.86146021]], w0=74.06780585492619, w1=11.034894865989076\n",
      "Gradient Descent(28/149): loss=[[131.86146021]], w0=74.06780585492632, w1=11.034894865989099\n",
      "Gradient Descent(29/149): loss=[[131.86146021]], w0=74.06780585492635, w1=11.0348948659891\n",
      "Gradient Descent(30/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(31/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(32/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(33/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(34/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(35/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(36/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(37/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(38/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(39/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(40/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(41/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(42/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(43/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(44/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(45/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(46/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(47/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(48/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(49/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(50/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(51/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(52/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(53/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(54/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(55/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(56/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(57/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(58/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(59/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(60/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(61/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(62/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(63/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(64/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(65/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(66/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(67/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(68/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(69/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(70/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(71/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(72/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(73/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(74/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(75/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(76/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(77/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(78/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(79/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(80/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(81/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(82/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(83/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(84/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(85/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(86/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(87/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(88/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(89/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(90/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(91/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(92/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(93/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(94/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(95/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(96/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(97/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(98/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(99/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(100/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(101/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(102/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(103/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(104/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(105/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(106/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(107/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(108/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(109/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(110/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(111/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(112/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(113/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(114/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(115/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(116/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(117/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(118/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(119/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(120/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(121/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(122/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(123/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(124/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(125/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(126/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(127/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(128/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(129/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(130/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(131/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(132/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(133/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(134/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(135/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(136/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(137/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(138/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(139/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(140/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(141/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(142/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(143/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(144/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(145/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(146/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(147/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(148/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(149/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "[74.06780585 11.03489487]\n",
      "[1.2  1.21 1.22 1.23 1.24 1.25 1.26 1.27 1.28 1.29 1.3  1.31 1.32 1.33\n",
      " 1.34 1.35 1.36 1.37 1.38 1.39 1.4  1.41 1.42 1.43 1.44 1.45 1.46 1.47\n",
      " 1.48 1.49 1.5  1.51 1.52 1.53 1.54 1.55 1.56 1.57 1.58 1.59 1.6  1.61\n",
      " 1.62 1.63 1.64 1.65 1.66 1.67 1.68 1.69 1.7  1.71 1.72 1.73 1.74 1.75\n",
      " 1.76 1.77 1.78 1.79 1.8  1.81 1.82 1.83 1.84 1.85 1.86 1.87 1.88 1.89\n",
      " 1.9  1.91 1.92 1.93 1.94 1.95 1.96 1.97 1.98 1.99]\n",
      "[ 30.39642599  31.34675742  32.29708884  33.24742026  34.19775168\n",
      "  35.1480831   36.09841452  37.04874594  37.99907736  38.94940878\n",
      "  39.89974021  40.85007163  41.80040305  42.75073447  43.70106589\n",
      "  44.65139731  45.60172873  46.55206015  47.50239157  48.45272299\n",
      "  49.40305442  50.35338584  51.30371726  52.25404868  53.2043801\n",
      "  54.15471152  55.10504294  56.05537436  57.00570578  57.95603721\n",
      "  58.90636863  59.85670005  60.80703147  61.75736289  62.70769431\n",
      "  63.65802573  64.60835715  65.55868857  66.50902     67.45935142\n",
      "  68.40968284  69.36001426  70.31034568  71.2606771   72.21100852\n",
      "  73.16133994  74.11167136  75.06200279  76.01233421  76.96266563\n",
      "  77.91299705  78.86332847  79.81365989  80.76399131  81.71432273\n",
      "  82.66465415  83.61498558  84.565317    85.51564842  86.46597984\n",
      "  87.41631126  88.36664268  89.3169741   90.26730552  91.21763694\n",
      "  92.16796837  93.11829979  94.06863121  95.01896263  95.96929405\n",
      "  96.91962547  97.86995689  98.82028831  99.77061973 100.72095115\n",
      " 101.67128258 102.621614   103.57194542 104.52227684 105.47260826]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1069a9cf8>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "plt.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.7\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "w = sgd_ws[-1]\n",
    "print(w)\n",
    "def prediction(w0, w1, mean_x, std_x):\n",
    "    \"\"\"Get the regression line from the model.\"\"\"\n",
    "    x = np.arange(1.2, 2, 0.01)\n",
    "    x_normalized = (x - mean_x) / std_x\n",
    "    return x, w0 + w1 * x_normalized\n",
    "\n",
    "pred_x, pred_y = prediction(\n",
    "        w[0], w[1],\n",
    "        mean_x, std_x)\n",
    "print(pred_x)\n",
    "print(pred_y)\n",
    "plt.plot(pred_x, pred_y, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function `compute_loss(y, tx, w)` for the Mean Absolute Error cost function [here](#compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "    y = np.array([y]).T\n",
    "    w = np.array([w]).T\n",
    "    \n",
    "    e = y - np.matmul(tx,w)  \n",
    "    \n",
    "    return (1/y.shape[0]) * (np.sum(np.abs(e), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sub_gradient(y, tx, w):\n",
    "    e = y - np.matmul(tx,w)\n",
    "\n",
    "    if 0 in np.sign(e):\n",
    "        print(colored('Non-deferentiable point found!', 'red'))\n",
    "    \n",
    "    return (1/y.shape[0]) * (np.sign(e).T).dot(-tx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        grad = compute_sub_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/249): loss=[73.293922], w0=1.0, w1=-1.1596057447604836e-15\n",
      "Gradient Descent(1/249): loss=[72.293922], w0=2.0, w1=-2.319211489520967e-15\n",
      "Gradient Descent(2/249): loss=[71.293922], w0=3.0, w1=-3.4788172342814508e-15\n",
      "Gradient Descent(3/249): loss=[70.293922], w0=4.0, w1=-4.638422979041934e-15\n",
      "Gradient Descent(4/249): loss=[69.293922], w0=5.0, w1=-5.798028723802418e-15\n",
      "Gradient Descent(5/249): loss=[68.293922], w0=6.0, w1=-6.9576344685629015e-15\n",
      "Gradient Descent(6/249): loss=[67.293922], w0=7.0, w1=-8.117240213323385e-15\n",
      "Gradient Descent(7/249): loss=[66.293922], w0=8.0, w1=-9.276845958083869e-15\n",
      "Gradient Descent(8/249): loss=[65.293922], w0=9.0, w1=-1.0436451702844352e-14\n",
      "Gradient Descent(9/249): loss=[64.293922], w0=10.0, w1=-1.1596057447604836e-14\n",
      "Gradient Descent(10/249): loss=[63.293922], w0=11.0, w1=-1.275566319236532e-14\n",
      "Gradient Descent(11/249): loss=[62.293922], w0=12.0, w1=-1.3915268937125803e-14\n",
      "Gradient Descent(12/249): loss=[61.293922], w0=13.0, w1=-1.5074874681886285e-14\n",
      "Gradient Descent(13/249): loss=[60.293922], w0=14.0, w1=-1.6234480426646767e-14\n",
      "Gradient Descent(14/249): loss=[59.293922], w0=15.0, w1=-1.739408617140725e-14\n",
      "Gradient Descent(15/249): loss=[58.293922], w0=16.0, w1=-1.855369191616773e-14\n",
      "Gradient Descent(16/249): loss=[57.293922], w0=17.0, w1=-1.9713297660928213e-14\n",
      "Gradient Descent(17/249): loss=[56.293922], w0=18.0, w1=-2.0872903405688695e-14\n",
      "Gradient Descent(18/249): loss=[55.293922], w0=19.0, w1=-2.2032509150449177e-14\n",
      "Gradient Descent(19/249): loss=[54.293922], w0=20.0, w1=-2.319211489520966e-14\n",
      "Gradient Descent(20/249): loss=[53.293922], w0=21.0, w1=-2.435172063997014e-14\n",
      "Gradient Descent(21/249): loss=[52.293922], w0=22.0, w1=-2.5511326384730623e-14\n",
      "Gradient Descent(22/249): loss=[51.293922], w0=23.0, w1=-2.6670932129491105e-14\n",
      "Gradient Descent(23/249): loss=[50.293922], w0=24.0, w1=-2.7830537874251587e-14\n",
      "Gradient Descent(24/249): loss=[49.293922], w0=25.0, w1=-2.899014361901207e-14\n",
      "Gradient Descent(25/249): loss=[48.293922], w0=26.0, w1=-3.014974936377255e-14\n",
      "Gradient Descent(26/249): loss=[47.293922], w0=27.0, w1=-3.1309355108533033e-14\n",
      "Gradient Descent(27/249): loss=[46.293922], w0=28.0, w1=-3.2468960853293515e-14\n",
      "Gradient Descent(28/249): loss=[45.293922], w0=29.0, w1=-3.3628566598054e-14\n",
      "Gradient Descent(29/249): loss=[44.293922], w0=30.0, w1=-3.478817234281448e-14\n",
      "Gradient Descent(30/249): loss=[43.29404723], w0=30.9998, w1=0.0006292368146911259\n",
      "Gradient Descent(31/249): loss=[42.29444679], w0=31.9996, w1=0.00125847362941704\n",
      "Gradient Descent(32/249): loss=[41.29498197], w0=32.9992, w1=0.0024439061264849215\n",
      "Gradient Descent(33/249): loss=[40.29589619], w0=33.9986, w1=0.004240187414517368\n",
      "Gradient Descent(34/249): loss=[39.29728463], w0=34.997800000000005, w1=0.00650507698654601\n",
      "Gradient Descent(35/249): loss=[38.29887886], w0=35.99700000000001, w1=0.008769966558574652\n",
      "Gradient Descent(36/249): loss=[37.30081383], w0=36.995400000000004, w1=0.01308578740315984\n",
      "Gradient Descent(37/249): loss=[36.30407251], w0=37.9936, w1=0.01793489457122146\n",
      "Gradient Descent(38/249): loss=[35.30782716], w0=38.9914, w1=0.023541285326675238\n",
      "Gradient Descent(39/249): loss=[34.31273699], w0=39.988, w1=0.03223740311282193\n",
      "Gradient Descent(40/249): loss=[33.32024634], w0=40.9832, w1=0.04380098780236799\n",
      "Gradient Descent(41/249): loss=[32.33036204], w0=41.9766, w1=0.059889714094082544\n",
      "Gradient Descent(42/249): loss=[31.34482649], w0=42.9664, w1=0.08351414257869323\n",
      "Gradient Descent(43/249): loss=[30.36607852], w0=43.9532, w1=0.11321535971099142\n",
      "Gradient Descent(44/249): loss=[29.39349328], w0=44.935, w1=0.1528681354036929\n",
      "Gradient Descent(45/249): loss=[28.4306269], w0=45.9114, w1=0.20292591469653748\n",
      "Gradient Descent(46/249): loss=[27.47803565], w0=46.881, w1=0.2654625075922855\n",
      "Gradient Descent(47/249): loss=[26.53773447], w0=47.8416, w1=0.3431967358723413\n",
      "Gradient Descent(48/249): loss=[25.61423189], w0=48.7904, w1=0.43977524069308305\n",
      "Gradient Descent(49/249): loss=[24.70767685], w0=49.7314, w1=0.5496584766727547\n",
      "Gradient Descent(50/249): loss=[23.81454384], w0=50.6606, w1=0.6780515686414907\n",
      "Gradient Descent(51/249): loss=[22.93951115], w0=51.576800000000006, w1=0.8262619540810324\n",
      "Gradient Descent(52/249): loss=[22.08294699], w0=52.47880000000001, w1=0.9944166163992859\n",
      "Gradient Descent(53/249): loss=[21.24620624], w0=53.36560000000001, w1=1.1841419635899146\n",
      "Gradient Descent(54/249): loss=[20.42860885], w0=54.23800000000001, w1=1.3934551808909061\n",
      "Gradient Descent(55/249): loss=[19.62805637], w0=55.094800000000006, w1=1.6235157851730575\n",
      "Gradient Descent(56/249): loss=[18.84613862], w0=55.933400000000006, w1=1.8755621770087216\n",
      "Gradient Descent(57/249): loss=[18.08428139], w0=56.755, w1=2.1478502149191305\n",
      "Gradient Descent(58/249): loss=[17.34012138], w0=57.5568, w1=2.439876465627976\n",
      "Gradient Descent(59/249): loss=[16.61615195], w0=58.3404, w1=2.749633145454438\n",
      "Gradient Descent(60/249): loss=[15.91037668], w0=59.107800000000005, w1=3.0753504464732315\n",
      "Gradient Descent(61/249): loss=[15.22025638], w0=59.8556, w1=3.4198611616557333\n",
      "Gradient Descent(62/249): loss=[14.54679885], w0=60.5818, w1=3.7811422773727172\n",
      "Gradient Descent(63/249): loss=[13.89292573], w0=61.291000000000004, w1=4.154506840793032\n",
      "Gradient Descent(64/249): loss=[13.25359696], w0=61.9846, w1=4.540211089623438\n",
      "Gradient Descent(65/249): loss=[12.62815167], w0=62.6592, w1=4.939521219024335\n",
      "Gradient Descent(66/249): loss=[12.01805894], w0=63.315799999999996, w1=5.3490262242506175\n",
      "Gradient Descent(67/249): loss=[11.42441572], w0=63.9522, w1=5.767666921524737\n",
      "Gradient Descent(68/249): loss=[10.84837917], w0=64.5726, w1=6.191682366928532\n",
      "Gradient Descent(69/249): loss=[10.28981676], w0=65.1704, w1=6.6214096457253175\n",
      "Gradient Descent(70/249): loss=[9.75327659], w0=65.7474, w1=7.052769774753377\n",
      "Gradient Descent(71/249): loss=[9.23882072], w0=66.3042, w1=7.487048192851909\n",
      "Gradient Descent(72/249): loss=[8.74526754], w0=66.84259999999999, w1=7.922078399996176\n",
      "Gradient Descent(73/249): loss=[8.27172445], w0=67.35839999999999, w1=8.357239821080816\n",
      "Gradient Descent(74/249): loss=[7.82085911], w0=67.86119999999998, w1=8.787247234290762\n",
      "Gradient Descent(75/249): loss=[7.38912965], w0=68.34159999999999, w1=9.214951838263575\n",
      "Gradient Descent(76/249): loss=[6.9813445], w0=68.80399999999999, w1=9.633814592134177\n",
      "Gradient Descent(77/249): loss=[6.59828083], w0=69.24659999999999, w1=10.041353746808136\n",
      "Gradient Descent(78/249): loss=[6.24469296], w0=69.66119999999998, w1=10.43539831885831\n",
      "Gradient Descent(79/249): loss=[5.92553103], w0=70.05179999999999, w1=10.812453357711929\n",
      "Gradient Descent(80/249): loss=[5.63966213], w0=70.41659999999999, w1=11.166353221907606\n",
      "Gradient Descent(81/249): loss=[5.38983018], w0=70.75599999999999, w1=11.49681503502124\n",
      "Gradient Descent(82/249): loss=[5.17355131], w0=71.06879999999998, w1=11.799454239654489\n",
      "Gradient Descent(83/249): loss=[4.99356564], w0=71.35119999999998, w1=12.067923790406969\n",
      "Gradient Descent(84/249): loss=[4.85059605], w0=71.60479999999998, w1=12.29930877189011\n",
      "Gradient Descent(85/249): loss=[4.73983763], w0=71.82979999999998, w1=12.500289118910514\n",
      "Gradient Descent(86/249): loss=[4.65560171], w0=72.02499999999998, w1=12.673699380655929\n",
      "Gradient Descent(87/249): loss=[4.59204698], w0=72.19679999999998, w1=12.81738241109655\n",
      "Gradient Descent(88/249): loss=[4.54605853], w0=72.34319999999998, w1=12.939011888496427\n",
      "Gradient Descent(89/249): loss=[4.5125115], w0=72.47059999999998, w1=13.040456747689321\n",
      "Gradient Descent(90/249): loss=[4.48772401], w0=72.58079999999998, w1=13.125648638411874\n",
      "Gradient Descent(91/249): loss=[4.4698107], w0=72.67459999999998, w1=13.194977256351436\n",
      "Gradient Descent(92/249): loss=[4.45721224], w0=72.75459999999998, w1=13.252674991393523\n",
      "Gradient Descent(93/249): loss=[4.44814859], w0=72.82479999999998, w1=13.300940989067264\n",
      "Gradient Descent(94/249): loss=[4.44140433], w0=72.88519999999998, w1=13.34189528446737\n",
      "Gradient Descent(95/249): loss=[4.43662363], w0=72.93579999999999, w1=13.373666425828816\n",
      "Gradient Descent(96/249): loss=[4.43331748], w0=72.97879999999999, w1=13.398911941055944\n",
      "Gradient Descent(97/249): loss=[4.43099397], w0=73.017, w1=13.421088544625697\n",
      "Gradient Descent(98/249): loss=[4.42922792], w0=73.0486, w1=13.437948493680754\n",
      "Gradient Descent(99/249): loss=[4.42801909], w0=73.0762, w1=13.453593486960756\n",
      "Gradient Descent(100/249): loss=[4.42707028], w0=73.1002, w1=13.467616680599612\n",
      "Gradient Descent(101/249): loss=[4.42636832], w0=73.1198, w1=13.47912877415011\n",
      "Gradient Descent(102/249): loss=[4.42589534], w0=73.1364, w1=13.488221858561687\n",
      "Gradient Descent(103/249): loss=[4.42556287], w0=73.1514, w1=13.496053547459962\n",
      "Gradient Descent(104/249): loss=[4.42529225], w0=73.1646, w1=13.502759696367878\n",
      "Gradient Descent(105/249): loss=[4.4250846], w0=73.1758, w1=13.508075665474191\n",
      "Gradient Descent(106/249): loss=[4.42494641], w0=73.18459999999999, w1=13.51218441323186\n",
      "Gradient Descent(107/249): loss=[4.4248585], w0=73.19239999999999, w1=13.515821473377963\n",
      "Gradient Descent(108/249): loss=[4.42479113], w0=73.19879999999999, w1=13.518511994654597\n",
      "Gradient Descent(109/249): loss=[4.42475131], w0=73.20339999999999, w1=13.51963229021564\n",
      "Gradient Descent(110/249): loss=[4.42472932], w0=73.20779999999999, w1=13.520597578075435\n",
      "Gradient Descent(111/249): loss=[4.42470903], w0=73.2122, w1=13.52156286593523\n",
      "Gradient Descent(112/249): loss=[4.42468941], w0=73.2164, w1=13.522698116649215\n",
      "Gradient Descent(113/249): loss=[4.42467245], w0=73.21979999999999, w1=13.52316340081199\n",
      "Gradient Descent(114/249): loss=[4.42466097], w0=73.22279999999999, w1=13.523505749586366\n",
      "Gradient Descent(115/249): loss=[4.42465306], w0=73.22479999999999, w1=13.523035014160062\n",
      "Gradient Descent(116/249): loss=[4.42464884], w0=73.22679999999998, w1=13.522564278733757\n",
      "Gradient Descent(117/249): loss=[4.42464462], w0=73.22879999999998, w1=13.522093543307452\n",
      "Gradient Descent(118/249): loss=[4.42464096], w0=73.23039999999997, w1=13.522276375565555\n",
      "Gradient Descent(119/249): loss=[4.42463842], w0=73.23179999999998, w1=13.522521799654925\n",
      "Gradient Descent(120/249): loss=[4.42463667], w0=73.23279999999998, w1=13.523281181718403\n",
      "Gradient Descent(121/249): loss=[4.42463528], w0=73.23359999999998, w1=13.523959689588368\n",
      "Gradient Descent(122/249): loss=[4.42463428], w0=73.23439999999998, w1=13.523889972087819\n",
      "Gradient Descent(123/249): loss=[4.42463373], w0=73.23499999999999, w1=13.524218516909647\n",
      "Gradient Descent(124/249): loss=[4.42463326], w0=73.23559999999999, w1=13.524547061731475\n",
      "Gradient Descent(125/249): loss=[4.42463279], w0=73.2362, w1=13.524875606553303\n",
      "Gradient Descent(126/249): loss=[4.42463239], w0=73.2366, w1=13.525098088968234\n",
      "Gradient Descent(127/249): loss=[4.42463218], w0=73.237, w1=13.525320571383164\n",
      "Gradient Descent(128/249): loss=[4.42463202], w0=73.2372, w1=13.525175829834637\n",
      "Gradient Descent(129/249): loss=[4.42463195], w0=73.23740000000001, w1=13.525031088286111\n",
      "Gradient Descent(130/249): loss=[4.42463189], w0=73.23760000000001, w1=13.524886346737585\n",
      "Gradient Descent(131/249): loss=[4.42463183], w0=73.23780000000002, w1=13.524741605189059\n",
      "Gradient Descent(132/249): loss=[4.42463178], w0=73.23820000000002, w1=13.524964087603989\n",
      "Gradient Descent(133/249): loss=[4.42463172], w0=73.23840000000003, w1=13.524819346055462\n",
      "Gradient Descent(134/249): loss=[4.42463166], w0=73.23860000000003, w1=13.524674604506936\n",
      "Gradient Descent(135/249): loss=[4.42463162], w0=73.23860000000003, w1=13.524625822866142\n",
      "Gradient Descent(136/249): loss=[4.42463162], w0=73.23860000000003, w1=13.524577041225347\n",
      "Gradient Descent(137/249): loss=[4.42463162], w0=73.23860000000003, w1=13.524528259584553\n",
      "Gradient Descent(138/249): loss=[4.42463162], w0=73.23860000000003, w1=13.524479477943759\n",
      "Gradient Descent(139/249): loss=[4.42463162], w0=73.23860000000003, w1=13.524430696302964\n",
      "Gradient Descent(140/249): loss=[4.42463161], w0=73.23860000000003, w1=13.52438191466217\n",
      "Gradient Descent(141/249): loss=[4.42463161], w0=73.23860000000003, w1=13.524333133021376\n",
      "Gradient Descent(142/249): loss=[4.42463161], w0=73.23860000000003, w1=13.524284351380581\n",
      "Gradient Descent(143/249): loss=[4.42463162], w0=73.23880000000004, w1=13.524602793703245\n",
      "Gradient Descent(144/249): loss=[4.42463162], w0=73.23880000000004, w1=13.52455401206245\n",
      "Gradient Descent(145/249): loss=[4.42463162], w0=73.23880000000004, w1=13.524505230421656\n",
      "Gradient Descent(146/249): loss=[4.42463162], w0=73.23880000000004, w1=13.524456448780862\n",
      "Gradient Descent(147/249): loss=[4.42463161], w0=73.23880000000004, w1=13.524407667140068\n",
      "Gradient Descent(148/249): loss=[4.42463161], w0=73.23880000000004, w1=13.524358885499273\n",
      "Gradient Descent(149/249): loss=[4.42463161], w0=73.23880000000004, w1=13.524310103858479\n",
      "Gradient Descent(150/249): loss=[4.42463161], w0=73.23880000000004, w1=13.524261322217685\n",
      "Gradient Descent(151/249): loss=[4.4246316], w0=73.23880000000004, w1=13.52421254057689\n",
      "Gradient Descent(152/249): loss=[4.42463161], w0=73.23900000000005, w1=13.524530982899554\n",
      "Gradient Descent(153/249): loss=[4.42463162], w0=73.23900000000005, w1=13.52448220125876\n",
      "Gradient Descent(154/249): loss=[4.42463162], w0=73.23900000000005, w1=13.524433419617965\n",
      "Gradient Descent(155/249): loss=[4.42463161], w0=73.23900000000005, w1=13.524384637977171\n",
      "Gradient Descent(156/249): loss=[4.42463161], w0=73.23900000000005, w1=13.524335856336377\n",
      "Gradient Descent(157/249): loss=[4.42463161], w0=73.23900000000005, w1=13.524287074695582\n",
      "Gradient Descent(158/249): loss=[4.42463161], w0=73.23900000000005, w1=13.524238293054788\n",
      "Gradient Descent(159/249): loss=[4.4246316], w0=73.23900000000005, w1=13.524189511413994\n",
      "Gradient Descent(160/249): loss=[4.4246316], w0=73.23900000000005, w1=13.5241407297732\n",
      "Gradient Descent(161/249): loss=[4.4246316], w0=73.23900000000005, w1=13.524091948132405\n",
      "Gradient Descent(162/249): loss=[4.4246316], w0=73.23920000000005, w1=13.524410390455069\n",
      "Gradient Descent(163/249): loss=[4.42463161], w0=73.23920000000005, w1=13.524361608814274\n",
      "Gradient Descent(164/249): loss=[4.42463161], w0=73.23920000000005, w1=13.52431282717348\n",
      "Gradient Descent(165/249): loss=[4.42463161], w0=73.23920000000005, w1=13.524264045532686\n",
      "Gradient Descent(166/249): loss=[4.4246316], w0=73.23920000000005, w1=13.524215263891891\n",
      "Gradient Descent(167/249): loss=[4.4246316], w0=73.23920000000005, w1=13.524166482251097\n",
      "Gradient Descent(168/249): loss=[4.4246316], w0=73.23920000000005, w1=13.524117700610303\n",
      "Gradient Descent(169/249): loss=[4.4246316], w0=73.23920000000005, w1=13.524068918969508\n",
      "Gradient Descent(170/249): loss=[4.4246316], w0=73.23920000000005, w1=13.524020137328714\n",
      "Gradient Descent(171/249): loss=[4.42463159], w0=73.23920000000005, w1=13.52397135568792\n",
      "Gradient Descent(172/249): loss=[4.4246316], w0=73.23940000000006, w1=13.524289798010583\n",
      "Gradient Descent(173/249): loss=[4.42463161], w0=73.23940000000006, w1=13.524241016369789\n",
      "Gradient Descent(174/249): loss=[4.4246316], w0=73.23940000000006, w1=13.524192234728995\n",
      "Gradient Descent(175/249): loss=[4.4246316], w0=73.23940000000006, w1=13.5241434530882\n",
      "Gradient Descent(176/249): loss=[4.4246316], w0=73.23940000000006, w1=13.524094671447406\n",
      "Gradient Descent(177/249): loss=[4.4246316], w0=73.23940000000006, w1=13.524045889806612\n",
      "Gradient Descent(178/249): loss=[4.42463159], w0=73.23940000000006, w1=13.523997108165817\n",
      "Gradient Descent(179/249): loss=[4.42463159], w0=73.23940000000006, w1=13.523948326525023\n",
      "Gradient Descent(180/249): loss=[4.42463159], w0=73.23940000000006, w1=13.523899544884229\n",
      "Gradient Descent(181/249): loss=[4.42463159], w0=73.23940000000006, w1=13.523850763243434\n",
      "Gradient Descent(182/249): loss=[4.4246316], w0=73.23960000000007, w1=13.524169205566098\n",
      "Gradient Descent(183/249): loss=[4.4246316], w0=73.23960000000007, w1=13.524120423925304\n",
      "Gradient Descent(184/249): loss=[4.4246316], w0=73.23960000000007, w1=13.52407164228451\n",
      "Gradient Descent(185/249): loss=[4.4246316], w0=73.23960000000007, w1=13.524022860643715\n",
      "Gradient Descent(186/249): loss=[4.42463159], w0=73.23960000000007, w1=13.52397407900292\n",
      "Gradient Descent(187/249): loss=[4.42463159], w0=73.23960000000007, w1=13.523925297362126\n",
      "Gradient Descent(188/249): loss=[4.42463159], w0=73.23960000000007, w1=13.523876515721332\n",
      "Gradient Descent(189/249): loss=[4.42463159], w0=73.23960000000007, w1=13.523827734080538\n",
      "Gradient Descent(190/249): loss=[4.42463158], w0=73.23960000000007, w1=13.523778952439743\n",
      "Gradient Descent(191/249): loss=[4.42463158], w0=73.23980000000007, w1=13.524097394762407\n",
      "Gradient Descent(192/249): loss=[4.4246316], w0=73.23980000000007, w1=13.524048613121613\n",
      "Gradient Descent(193/249): loss=[4.42463159], w0=73.23980000000007, w1=13.523999831480818\n",
      "Gradient Descent(194/249): loss=[4.42463159], w0=73.23980000000007, w1=13.523951049840024\n",
      "Gradient Descent(195/249): loss=[4.42463159], w0=73.23980000000007, w1=13.52390226819923\n",
      "Gradient Descent(196/249): loss=[4.42463159], w0=73.23980000000007, w1=13.523853486558435\n",
      "Gradient Descent(197/249): loss=[4.42463158], w0=73.23980000000007, w1=13.52380470491764\n",
      "Gradient Descent(198/249): loss=[4.42463158], w0=73.23980000000007, w1=13.523755923276847\n",
      "Gradient Descent(199/249): loss=[4.42463158], w0=73.23980000000007, w1=13.523707141636052\n",
      "Gradient Descent(200/249): loss=[4.42463158], w0=73.23980000000007, w1=13.523658359995258\n",
      "Gradient Descent(201/249): loss=[4.42463158], w0=73.24000000000008, w1=13.523976802317922\n",
      "Gradient Descent(202/249): loss=[4.42463159], w0=73.24000000000008, w1=13.523928020677127\n",
      "Gradient Descent(203/249): loss=[4.42463159], w0=73.24000000000008, w1=13.523879239036333\n",
      "Gradient Descent(204/249): loss=[4.42463159], w0=73.24000000000008, w1=13.523830457395539\n",
      "Gradient Descent(205/249): loss=[4.42463158], w0=73.24000000000008, w1=13.523781675754744\n",
      "Gradient Descent(206/249): loss=[4.42463158], w0=73.24000000000008, w1=13.52373289411395\n",
      "Gradient Descent(207/249): loss=[4.42463158], w0=73.24000000000008, w1=13.523684112473155\n",
      "Gradient Descent(208/249): loss=[4.42463158], w0=73.24000000000008, w1=13.523635330832361\n",
      "Gradient Descent(209/249): loss=[4.42463157], w0=73.24000000000008, w1=13.523586549191567\n",
      "Gradient Descent(210/249): loss=[4.42463157], w0=73.24000000000008, w1=13.523537767550772\n",
      "Gradient Descent(211/249): loss=[4.42463158], w0=73.24020000000009, w1=13.523856209873436\n",
      "Gradient Descent(212/249): loss=[4.42463158], w0=73.24020000000009, w1=13.523807428232642\n",
      "Gradient Descent(213/249): loss=[4.42463158], w0=73.24020000000009, w1=13.523758646591848\n",
      "Gradient Descent(214/249): loss=[4.42463158], w0=73.24020000000009, w1=13.523709864951053\n",
      "Gradient Descent(215/249): loss=[4.42463158], w0=73.24020000000009, w1=13.523661083310259\n",
      "Gradient Descent(216/249): loss=[4.42463158], w0=73.24020000000009, w1=13.523612301669464\n",
      "Gradient Descent(217/249): loss=[4.42463157], w0=73.24020000000009, w1=13.52356352002867\n",
      "Gradient Descent(218/249): loss=[4.42463157], w0=73.24020000000009, w1=13.523514738387876\n",
      "Gradient Descent(219/249): loss=[4.42463157], w0=73.24020000000009, w1=13.523465956747081\n",
      "Gradient Descent(220/249): loss=[4.42463157], w0=73.24020000000009, w1=13.523417175106287\n",
      "Gradient Descent(221/249): loss=[4.42463158], w0=73.2404000000001, w1=13.523735617428951\n",
      "Gradient Descent(222/249): loss=[4.42463158], w0=73.2404000000001, w1=13.523686835788157\n",
      "Gradient Descent(223/249): loss=[4.42463158], w0=73.2404000000001, w1=13.523638054147362\n",
      "Gradient Descent(224/249): loss=[4.42463157], w0=73.2404000000001, w1=13.523589272506568\n",
      "Gradient Descent(225/249): loss=[4.42463157], w0=73.2404000000001, w1=13.523540490865773\n",
      "Gradient Descent(226/249): loss=[4.42463157], w0=73.2404000000001, w1=13.52349170922498\n",
      "Gradient Descent(227/249): loss=[4.42463157], w0=73.2404000000001, w1=13.523442927584185\n",
      "Gradient Descent(228/249): loss=[4.42463156], w0=73.2404000000001, w1=13.52339414594339\n",
      "Gradient Descent(229/249): loss=[4.42463156], w0=73.2404000000001, w1=13.523345364302596\n",
      "Gradient Descent(230/249): loss=[4.42463156], w0=73.2406000000001, w1=13.52366380662526\n",
      "Gradient Descent(231/249): loss=[4.42463158], w0=73.2406000000001, w1=13.523615024984466\n",
      "Gradient Descent(232/249): loss=[4.42463157], w0=73.2406000000001, w1=13.523566243343671\n",
      "Gradient Descent(233/249): loss=[4.42463157], w0=73.2406000000001, w1=13.523517461702877\n",
      "Gradient Descent(234/249): loss=[4.42463157], w0=73.2406000000001, w1=13.523468680062082\n",
      "Gradient Descent(235/249): loss=[4.42463157], w0=73.2406000000001, w1=13.523419898421288\n",
      "Gradient Descent(236/249): loss=[4.42463156], w0=73.2406000000001, w1=13.523371116780494\n",
      "Gradient Descent(237/249): loss=[4.42463156], w0=73.2406000000001, w1=13.5233223351397\n",
      "Gradient Descent(238/249): loss=[4.42463156], w0=73.2406000000001, w1=13.523273553498905\n",
      "Gradient Descent(239/249): loss=[4.42463156], w0=73.2406000000001, w1=13.52322477185811\n",
      "Gradient Descent(240/249): loss=[4.42463156], w0=73.2408000000001, w1=13.523543214180775\n",
      "Gradient Descent(241/249): loss=[4.42463157], w0=73.2408000000001, w1=13.52349443253998\n",
      "Gradient Descent(242/249): loss=[4.42463157], w0=73.2408000000001, w1=13.523445650899186\n",
      "Gradient Descent(243/249): loss=[4.42463156], w0=73.2408000000001, w1=13.523396869258391\n",
      "Gradient Descent(244/249): loss=[4.42463156], w0=73.2408000000001, w1=13.523348087617597\n",
      "Gradient Descent(245/249): loss=[4.42463156], w0=73.2408000000001, w1=13.523299305976803\n",
      "Gradient Descent(246/249): loss=[4.42463156], w0=73.2408000000001, w1=13.523250524336008\n",
      "Gradient Descent(247/249): loss=[4.42463156], w0=73.2408000000001, w1=13.523201742695214\n",
      "Gradient Descent(248/249): loss=[4.42463155], w0=73.2408000000001, w1=13.52315296105442\n",
      "Gradient Descent(249/249): loss=[4.42463155], w0=73.2408000000001, w1=13.523104179413625\n",
      "Gradient Descent: execution time=0.227 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 250\n",
    "gamma = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "sub_gradient_losses, sub_gradient_ws = sub_gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66231e500da4497867c81804d26f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=101, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sub_gradient_losses, sub_gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=[[5739.67022907]], w0=51.84746409844842, w1=7.7244264061924195\n",
      "Gradient Descent(1/149): loss=[[636.5642494]], w0=67.40170332798297, w1=10.041754328050114\n",
      "Gradient Descent(2/149): loss=[[177.28471123]], w0=72.06797509684336, w1=10.736952704607411\n",
      "Gradient Descent(3/149): loss=[[135.9495528]], w0=73.46785662750146, w1=10.945512217574597\n",
      "Gradient Descent(4/149): loss=[[132.22938854]], w0=73.88782108669889, w1=11.00808007146475\n",
      "Gradient Descent(5/149): loss=[[131.89457376]], w0=74.01381042445813, w1=11.026850427631798\n",
      "Gradient Descent(6/149): loss=[[131.86444042]], w0=74.0516072257859, w1=11.032481534481914\n",
      "Gradient Descent(7/149): loss=[[131.86172842]], w0=74.06294626618423, w1=11.034170866536945\n",
      "Gradient Descent(8/149): loss=[[131.86148434]], w0=74.06634797830372, w1=11.034677666153454\n",
      "Gradient Descent(9/149): loss=[[131.86146238]], w0=74.06736849193958, w1=11.034829706038408\n",
      "Gradient Descent(10/149): loss=[[131.8614604]], w0=74.06767464603033, w1=11.034875318003895\n",
      "Gradient Descent(11/149): loss=[[131.86146022]], w0=74.06776649225755, w1=11.034889001593541\n",
      "Gradient Descent(12/149): loss=[[131.86146021]], w0=74.06779404612573, w1=11.034893106670431\n",
      "Gradient Descent(13/149): loss=[[131.86146021]], w0=74.06780231228618, w1=11.034894338193501\n",
      "Gradient Descent(14/149): loss=[[131.86146021]], w0=74.06780479213431, w1=11.034894707650421\n",
      "Gradient Descent(15/149): loss=[[131.86146021]], w0=74.06780553608874, w1=11.034894818487496\n",
      "Gradient Descent(16/149): loss=[[131.86146021]], w0=74.06780575927507, w1=11.03489485173862\n",
      "Gradient Descent(17/149): loss=[[131.86146021]], w0=74.06780582623098, w1=11.034894861713957\n",
      "Gradient Descent(18/149): loss=[[131.86146021]], w0=74.06780584631775, w1=11.034894864706557\n",
      "Gradient Descent(19/149): loss=[[131.86146021]], w0=74.06780585234378, w1=11.034894865604338\n",
      "Gradient Descent(20/149): loss=[[131.86146021]], w0=74.06780585415159, w1=11.034894865873675\n",
      "Gradient Descent(21/149): loss=[[131.86146021]], w0=74.06780585469393, w1=11.034894865954474\n",
      "Gradient Descent(22/149): loss=[[131.86146021]], w0=74.06780585485663, w1=11.034894865978712\n",
      "Gradient Descent(23/149): loss=[[131.86146021]], w0=74.06780585490544, w1=11.034894865985985\n",
      "Gradient Descent(24/149): loss=[[131.86146021]], w0=74.0678058549201, w1=11.034894865988166\n",
      "Gradient Descent(25/149): loss=[[131.86146021]], w0=74.06780585492449, w1=11.034894865988822\n",
      "Gradient Descent(26/149): loss=[[131.86146021]], w0=74.06780585492581, w1=11.034894865989015\n",
      "Gradient Descent(27/149): loss=[[131.86146021]], w0=74.06780585492619, w1=11.034894865989076\n",
      "Gradient Descent(28/149): loss=[[131.86146021]], w0=74.06780585492632, w1=11.034894865989099\n",
      "Gradient Descent(29/149): loss=[[131.86146021]], w0=74.06780585492635, w1=11.0348948659891\n",
      "Gradient Descent(30/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(31/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(32/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(33/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(34/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(35/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(36/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(37/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(38/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(39/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(40/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(41/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(42/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(43/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(44/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(45/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(46/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(47/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(48/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(49/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(50/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(51/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(52/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(53/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(54/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(55/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(56/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(57/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(58/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(59/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(60/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(61/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(62/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(63/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(64/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(65/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(66/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(67/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(68/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(69/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(70/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(71/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(72/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(73/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(74/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(75/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(76/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(77/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(78/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(79/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(80/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(81/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(82/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(83/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(84/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(85/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(86/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(87/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(88/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(89/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(90/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(91/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(92/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(93/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(94/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(95/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(96/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(97/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(98/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(99/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(100/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(101/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(102/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(103/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(104/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(105/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(106/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(107/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(108/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(109/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(110/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(111/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(112/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(113/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(114/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(115/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(116/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(117/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(118/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(119/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(120/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(121/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(122/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(123/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(124/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(125/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(126/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(127/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(128/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(129/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(130/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(131/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(132/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(133/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(134/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(135/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(136/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(137/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(138/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(139/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(140/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(141/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(142/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(143/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(144/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(145/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(146/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(147/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(148/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(149/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(0/149): loss=[74.06780585], w0=0.7, w1=6.109524327590712e-16\n",
      "Gradient Descent(1/149): loss=[73.36780585], w0=1.4, w1=1.2219048655181425e-15\n",
      "Gradient Descent(2/149): loss=[72.66780585], w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "Gradient Descent(3/149): loss=[71.96780585], w0=2.8, w1=2.443809731036285e-15\n",
      "Gradient Descent(4/149): loss=[71.26780585], w0=3.5, w1=3.054762163795356e-15\n",
      "Gradient Descent(5/149): loss=[70.56780585], w0=4.2, w1=3.665714596554428e-15\n",
      "Gradient Descent(6/149): loss=[69.86780585], w0=4.9, w1=4.276667029313499e-15\n",
      "Gradient Descent(7/149): loss=[69.16780585], w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "Gradient Descent(8/149): loss=[68.46780585], w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "Gradient Descent(9/149): loss=[67.76780585], w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "Gradient Descent(10/149): loss=[67.06780585], w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "Gradient Descent(11/149): loss=[66.36780585], w0=8.4, w1=7.331429193108857e-15\n",
      "Gradient Descent(12/149): loss=[65.66780585], w0=9.1, w1=7.942381625867928e-15\n",
      "Gradient Descent(13/149): loss=[64.96780585], w0=9.799999999999999, w1=8.553334058627e-15\n",
      "Gradient Descent(14/149): loss=[64.26780585], w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "Gradient Descent(15/149): loss=[63.56780585], w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "Gradient Descent(16/149): loss=[62.86780585], w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "Gradient Descent(17/149): loss=[62.16780585], w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "Gradient Descent(18/149): loss=[61.46780585], w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "Gradient Descent(19/149): loss=[60.76780585], w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "Gradient Descent(20/149): loss=[60.06780585], w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "Gradient Descent(21/149): loss=[59.36780585], w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "Gradient Descent(22/149): loss=[58.66780585], w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "Gradient Descent(23/149): loss=[57.96780585], w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "Gradient Descent(24/149): loss=[57.26780585], w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "Gradient Descent(25/149): loss=[56.56780585], w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "Gradient Descent(26/149): loss=[55.86780585], w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "Gradient Descent(27/149): loss=[55.16780585], w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "Gradient Descent(28/149): loss=[54.46780585], w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "Gradient Descent(29/149): loss=[53.76780585], w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "Gradient Descent(30/149): loss=[53.06780585], w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "Gradient Descent(31/149): loss=[52.36780585], w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "Gradient Descent(32/149): loss=[51.66780585], w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "Gradient Descent(33/149): loss=[50.96780585], w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "Gradient Descent(34/149): loss=[50.26780585], w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "Gradient Descent(35/149): loss=[49.56780585], w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "Gradient Descent(36/149): loss=[48.86780585], w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "Gradient Descent(37/149): loss=[48.16780585], w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "Gradient Descent(38/149): loss=[47.46780585], w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "Gradient Descent(39/149): loss=[46.76780585], w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "Gradient Descent(40/149): loss=[46.06780585], w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "Gradient Descent(41/149): loss=[45.36780585], w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "Gradient Descent(42/149): loss=[44.66780585], w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "Gradient Descent(43/149): loss=[43.96780585], w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "Gradient Descent(44/149): loss=[43.26780585], w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "Gradient Descent(45/149): loss=[42.56780585], w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "Gradient Descent(46/149): loss=[41.86780585], w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "Gradient Descent(47/149): loss=[41.16780585], w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "Gradient Descent(48/149): loss=[40.46780585], w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "Gradient Descent(49/149): loss=[39.76780585], w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "Gradient Descent(50/149): loss=[39.06780585], w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "Gradient Descent(51/149): loss=[38.36780585], w0=36.4, w1=3.176952650347169e-14\n",
      "Gradient Descent(52/149): loss=[37.66780585], w0=37.1, w1=3.238047893623076e-14\n",
      "Gradient Descent(53/149): loss=[36.96780585], w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "Gradient Descent(54/149): loss=[36.26780585], w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "Gradient Descent(55/149): loss=[35.56780585], w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "Gradient Descent(56/149): loss=[34.86780585], w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "Gradient Descent(57/149): loss=[34.16780585], w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "Gradient Descent(58/149): loss=[33.46780585], w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "Gradient Descent(59/149): loss=[32.76780585], w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "Gradient Descent(60/149): loss=[32.06780585], w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "Gradient Descent(61/149): loss=[31.36780585], w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "Gradient Descent(62/149): loss=[30.66780585], w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "Gradient Descent(63/149): loss=[29.96780585], w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "Gradient Descent(64/149): loss=[29.26780585], w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "Gradient Descent(65/149): loss=[28.56780585], w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "Gradient Descent(66/149): loss=[27.86780585], w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "Gradient Descent(67/149): loss=[27.17327021], w0=47.59306930693074, w1=0.011147845678271063\n",
      "Gradient Descent(68/149): loss=[26.49045156], w0=48.279207920792125, w1=0.03308574108989941\n",
      "Gradient Descent(69/149): loss=[25.81721232], w0=48.96534653465351, w1=0.05502363650152776\n",
      "Gradient Descent(70/149): loss=[25.15503943], w0=49.63069306930698, w1=0.10538326388307814\n",
      "Gradient Descent(71/149): loss=[24.52410341], w0=50.28910891089114, w1=0.16746568532793435\n",
      "Gradient Descent(72/149): loss=[23.89929535], w0=50.947524752475296, w1=0.22954810677279056\n",
      "Gradient Descent(73/149): loss=[23.28439293], w0=51.59207920792084, w1=0.31242512932747524\n",
      "Gradient Descent(74/149): loss=[22.68687644], w0=52.22277227722777, w1=0.4119501328839991\n",
      "Gradient Descent(75/149): loss=[22.10626757], w0=52.84653465346539, w1=0.5208167847923756\n",
      "Gradient Descent(76/149): loss=[21.53781883], w0=53.4564356435644, w1=0.6457900912635992\n",
      "Gradient Descent(77/149): loss=[20.98633987], w0=54.0594059405941, w1=0.7796904498577214\n",
      "Gradient Descent(78/149): loss=[20.44556094], w0=54.655445544554496, w1=0.9197570104995693\n",
      "Gradient Descent(79/149): loss=[19.91191016], w0=55.24455445544559, w1=1.0670920297849913\n",
      "Gradient Descent(80/149): loss=[19.38964409], w0=55.819801980198065, w1=1.2261255948210765\n",
      "Gradient Descent(81/149): loss=[18.88798906], w0=56.36732673267331, w1=1.410709342622213\n",
      "Gradient Descent(82/149): loss=[18.4159605], w0=56.900990099009945, w1=1.605853732220269\n",
      "Gradient Descent(83/149): loss=[17.95489854], w0=57.42772277227727, w1=1.808762802293962\n",
      "Gradient Descent(84/149): loss=[17.50575766], w0=57.933663366336674, w1=2.0285064197514697\n",
      "Gradient Descent(85/149): loss=[17.07495743], w0=58.43267326732677, w1=2.2494370848672776\n",
      "Gradient Descent(86/149): loss=[16.6529673], w0=58.91089108910895, w1=2.4837982986028337\n",
      "Gradient Descent(87/149): loss=[16.24854073], w0=59.382178217821824, w1=2.7260245553531504\n",
      "Gradient Descent(88/149): loss=[15.84910521], w0=59.83960396039608, w1=2.978742333469136\n",
      "Gradient Descent(89/149): loss=[15.46691979], w0=60.262376237623805, w1=3.251528669355438\n",
      "Gradient Descent(90/149): loss=[15.10829462], w0=60.67821782178222, w1=3.5270865794242794\n",
      "Gradient Descent(91/149): loss=[14.75489635], w0=61.087128712871326, w1=3.806459183951815\n",
      "Gradient Descent(92/149): loss=[14.40452896], w0=61.49603960396043, w1=4.085831788479351\n",
      "Gradient Descent(93/149): loss=[14.05578703], w0=61.891089108910926, w1=4.373839384328607\n",
      "Gradient Descent(94/149): loss=[13.71462091], w0=62.27920792079211, w1=4.666037469532047\n",
      "Gradient Descent(95/149): loss=[13.38123631], w0=62.65346534653469, w1=4.959829093241769\n",
      "Gradient Descent(96/149): loss=[13.05882162], w0=63.02079207920796, w1=5.25705719205664\n",
      "Gradient Descent(97/149): loss=[12.74025172], w0=63.38118811881192, w1=5.560434316352406\n",
      "Gradient Descent(98/149): loss=[12.42321889], w0=63.74158415841588, w1=5.863811440648173\n",
      "Gradient Descent(99/149): loss=[12.10756173], w0=64.08811881188123, w1=6.172402175278548\n",
      "Gradient Descent(100/149): loss=[11.8006221], w0=64.42772277227726, w1=6.486369310516498\n",
      "Gradient Descent(101/149): loss=[11.49504179], w0=64.7673267326733, w1=6.800336445754448\n",
      "Gradient Descent(102/149): loss=[11.18946149], w0=65.10693069306933, w1=7.114303580992399\n",
      "Gradient Descent(103/149): loss=[10.88388119], w0=65.44653465346536, w1=7.428270716230349\n",
      "Gradient Descent(104/149): loss=[10.58459341], w0=65.76534653465349, w1=7.747893210218626\n",
      "Gradient Descent(105/149): loss=[10.29581653], w0=66.070297029703, w1=8.073669686866905\n",
      "Gradient Descent(106/149): loss=[10.01135208], w0=66.37524752475251, w1=8.399446163515185\n",
      "Gradient Descent(107/149): loss=[9.72808433], w0=66.6663366336634, w1=8.73297028041739\n",
      "Gradient Descent(108/149): loss=[9.44812546], w0=66.9574257425743, w1=9.066494397319596\n",
      "Gradient Descent(109/149): loss=[9.1710411], w0=67.23465346534658, w1=9.39863031947029\n",
      "Gradient Descent(110/149): loss=[8.90365613], w0=67.51188118811886, w1=9.730766241620982\n",
      "Gradient Descent(111/149): loss=[8.63627116], w0=67.78910891089114, w1=10.062902163771675\n",
      "Gradient Descent(112/149): loss=[8.37615192], w0=68.06633663366343, w1=10.363999289979422\n",
      "Gradient Descent(113/149): loss=[8.14054084], w0=68.32970297029709, w1=10.660466909273612\n",
      "Gradient Descent(114/149): loss=[7.9185445], w0=68.59306930693076, w1=10.943174379960814\n",
      "Gradient Descent(115/149): loss=[7.70527973], w0=68.85643564356442, w1=11.225881850648015\n",
      "Gradient Descent(116/149): loss=[7.49369583], w0=69.11287128712878, w1=11.504395843582206\n",
      "Gradient Descent(117/149): loss=[7.28999241], w0=69.35544554455453, w1=11.78820189306775\n",
      "Gradient Descent(118/149): loss=[7.09723404], w0=69.58415841584166, w1=12.060911465190971\n",
      "Gradient Descent(119/149): loss=[6.91990529], w0=69.80594059405948, w1=12.324245668386048\n",
      "Gradient Descent(120/149): loss=[6.75057353], w0=70.0277227722773, w1=12.587579871581125\n",
      "Gradient Descent(121/149): loss=[6.58474481], w0=70.25643564356443, w1=12.824765405096484\n",
      "Gradient Descent(122/149): loss=[6.43034328], w0=70.47821782178225, w1=13.065616959310148\n",
      "Gradient Descent(123/149): loss=[6.27807148], w0=70.69306930693077, w1=13.302953389983912\n",
      "Gradient Descent(124/149): loss=[6.13366333], w0=70.89405940594067, w1=13.525403099312918\n",
      "Gradient Descent(125/149): loss=[6.0058408], w0=71.08811881188126, w1=13.742945617944212\n",
      "Gradient Descent(126/149): loss=[5.88502183], w0=71.27524752475254, w1=13.953548196006844\n",
      "Gradient Descent(127/149): loss=[5.77163525], w0=71.46237623762383, w1=14.164150774069476\n",
      "Gradient Descent(128/149): loss=[5.66716206], w0=71.62178217821788, w1=14.349779559473173\n",
      "Gradient Descent(129/149): loss=[5.58672677], w0=71.75346534653471, w1=14.51689010761231\n",
      "Gradient Descent(130/149): loss=[5.52384781], w0=71.87128712871292, w1=14.670791185324186\n",
      "Gradient Descent(131/149): loss=[5.48009371], w0=71.95445544554461, w1=14.780276456654521\n",
      "Gradient Descent(132/149): loss=[5.453088], w0=72.0376237623763, w1=14.889761727984856\n",
      "Gradient Descent(133/149): loss=[5.42739263], w0=72.10693069306937, w1=14.985916181776727\n",
      "Gradient Descent(134/149): loss=[5.40732245], w0=72.17623762376245, w1=15.082070635568597\n",
      "Gradient Descent(135/149): loss=[5.38725226], w0=72.24554455445552, w1=15.178225089360467\n",
      "Gradient Descent(136/149): loss=[5.37046078], w0=72.30099009900998, w1=15.25972348971591\n",
      "Gradient Descent(137/149): loss=[5.35740652], w0=72.34950495049513, w1=15.335091856448138\n",
      "Gradient Descent(138/149): loss=[5.34592926], w0=72.39801980198028, w1=15.410460223180365\n",
      "Gradient Descent(139/149): loss=[5.33571466], w0=72.43267326732682, w1=15.469961786755725\n",
      "Gradient Descent(140/149): loss=[5.33004391], w0=72.46039603960405, w1=15.51864528583281\n",
      "Gradient Descent(141/149): loss=[5.32567643], w0=72.48811881188128, w1=15.561592159086487\n",
      "Gradient Descent(142/149): loss=[5.32217673], w0=72.5019801980199, w1=15.597828332032526\n",
      "Gradient Descent(143/149): loss=[5.32011131], w0=72.52277227722782, w1=15.624722856626713\n",
      "Gradient Descent(144/149): loss=[5.31847828], w0=72.55049504950505, w1=15.642690329098\n",
      "Gradient Descent(145/149): loss=[5.31724005], w0=72.56435643564366, w1=15.664356578291091\n",
      "Gradient Descent(146/149): loss=[5.31640655], w0=72.58514851485158, w1=15.677095775361284\n",
      "Gradient Descent(147/149): loss=[5.31555712], w0=72.6059405940595, w1=15.689834972431477\n",
      "Gradient Descent(148/149): loss=[5.3147077], w0=72.62673267326743, w1=15.70257416950167\n",
      "Gradient Descent(149/149): loss=[5.31387688], w0=72.64059405940604, w1=15.72424041869476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1069e96d8>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare gradient and sub_gradient decents\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.grid()\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "ax2.grid()\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.7\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "def prediction(w0, w1, mean_x, std_x):\n",
    "    \"\"\"Get the regression line from the model.\"\"\"\n",
    "    x = np.arange(1.2, 2, 0.01)\n",
    "    x_normalized = (x - mean_x) / std_x\n",
    "    return x, w0 + w1 * x_normalized\n",
    "\n",
    "# Start GD.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "wgd = gd_ws[-1]\n",
    "\n",
    "pred_x, pred_y = prediction(\n",
    "        wgd[0], wgd[1],\n",
    "        mean_x, std_x)\n",
    "\n",
    "ax1.plot(pred_x, pred_y, 'r')\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = sub_gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "wsgd = sgd_ws[-1]\n",
    "\n",
    "pred_x, pred_y = prediction(\n",
    "        wsgd[0], wsgd[1],\n",
    "        mean_x, std_x)\n",
    "\n",
    "ax2.plot(pred_x, pred_y, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Sub_Gradiant_Descent with MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_sub_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    \n",
    "    return compute_sub_gradient(y, tx, w)\n",
    "    \n",
    "\n",
    "\n",
    "def stochastic_sub_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "      \n",
    "    gen = batch_iter(y, tx, batch_size)\n",
    "    \n",
    "    y = []\n",
    "    tx = []\n",
    "    for pair in gen:\n",
    "        y.append(pair[0])\n",
    "        tx.append(pair[1])\n",
    "        \n",
    "    y = np.asarray(y).T.reshape(100)\n",
    "    tx = np.asarray(tx).reshape((100,2))\n",
    "    \n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        grad = compute_stoch_sub_gradient(y, tx, w)\n",
    "\n",
    "        w = w - (gamma * grad)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/249): loss=[72.82343501], w0=1.0, w1=0.011982539511440895\n",
      "Gradient Descent(1/249): loss=[71.82329143], w0=2.0, w1=0.02396507902288179\n",
      "Gradient Descent(2/249): loss=[70.82314785], w0=3.0, w1=0.035947618534322684\n",
      "Gradient Descent(3/249): loss=[69.82300427], w0=4.0, w1=0.04793015804576358\n",
      "Gradient Descent(4/249): loss=[68.82286068], w0=5.0, w1=0.05991269755720448\n",
      "Gradient Descent(5/249): loss=[67.8227171], w0=6.0, w1=0.07189523706864537\n",
      "Gradient Descent(6/249): loss=[66.82257352], w0=7.0, w1=0.08387777658008626\n",
      "Gradient Descent(7/249): loss=[65.82242994], w0=8.0, w1=0.09586031609152715\n",
      "Gradient Descent(8/249): loss=[64.82228636], w0=9.0, w1=0.10784285560296804\n",
      "Gradient Descent(9/249): loss=[63.82214278], w0=10.0, w1=0.11982539511440893\n",
      "Gradient Descent(10/249): loss=[62.8219992], w0=11.0, w1=0.13180793462584983\n",
      "Gradient Descent(11/249): loss=[61.82185562], w0=12.0, w1=0.14379047413729074\n",
      "Gradient Descent(12/249): loss=[60.82171203], w0=13.0, w1=0.15577301364873164\n",
      "Gradient Descent(13/249): loss=[59.82156845], w0=14.0, w1=0.16775555316017254\n",
      "Gradient Descent(14/249): loss=[58.82142487], w0=15.0, w1=0.17973809267161345\n",
      "Gradient Descent(15/249): loss=[57.82128129], w0=16.0, w1=0.19172063218305435\n",
      "Gradient Descent(16/249): loss=[56.82113771], w0=17.0, w1=0.20370317169449526\n",
      "Gradient Descent(17/249): loss=[55.82099413], w0=18.0, w1=0.21568571120593616\n",
      "Gradient Descent(18/249): loss=[54.82085055], w0=19.0, w1=0.22766825071737706\n",
      "Gradient Descent(19/249): loss=[53.82070697], w0=20.0, w1=0.23965079022881797\n",
      "Gradient Descent(20/249): loss=[52.82056338], w0=21.0, w1=0.25163332974025887\n",
      "Gradient Descent(21/249): loss=[51.8204198], w0=22.0, w1=0.2636158692516998\n",
      "Gradient Descent(22/249): loss=[50.82027622], w0=23.0, w1=0.2755984087631407\n",
      "Gradient Descent(23/249): loss=[49.82013264], w0=24.0, w1=0.2875809482745816\n",
      "Gradient Descent(24/249): loss=[48.81998906], w0=25.0, w1=0.2995634877860225\n",
      "Gradient Descent(25/249): loss=[47.81984548], w0=26.0, w1=0.3115460272974634\n",
      "Gradient Descent(26/249): loss=[46.8197019], w0=27.0, w1=0.3235285668089043\n",
      "Gradient Descent(27/249): loss=[45.81955832], w0=28.0, w1=0.3355111063203452\n",
      "Gradient Descent(28/249): loss=[44.81941473], w0=29.0, w1=0.3474936458317861\n",
      "Gradient Descent(29/249): loss=[43.81927115], w0=30.0, w1=0.359476185343227\n",
      "Gradient Descent(30/249): loss=[42.81912757], w0=31.0, w1=0.3714587248546679\n",
      "Gradient Descent(31/249): loss=[41.81898399], w0=32.0, w1=0.3834412643661088\n",
      "Gradient Descent(32/249): loss=[40.81884041], w0=33.0, w1=0.3954238038775497\n",
      "Gradient Descent(33/249): loss=[39.81869683], w0=34.0, w1=0.4074063433889906\n",
      "Gradient Descent(34/249): loss=[38.81855325], w0=35.0, w1=0.4193888829004315\n",
      "Gradient Descent(35/249): loss=[37.81840967], w0=36.0, w1=0.43137142241187243\n",
      "Gradient Descent(36/249): loss=[36.81826608], w0=37.0, w1=0.44335396192331333\n",
      "Gradient Descent(37/249): loss=[35.8181225], w0=38.0, w1=0.45533650143475424\n",
      "Gradient Descent(38/249): loss=[34.81797892], w0=39.0, w1=0.46731904094619514\n",
      "Gradient Descent(39/249): loss=[33.81783534], w0=40.0, w1=0.47930158045763605\n",
      "Gradient Descent(40/249): loss=[32.81769176], w0=41.0, w1=0.49128411996907695\n",
      "Gradient Descent(41/249): loss=[31.81754818], w0=42.0, w1=0.5032666594805179\n",
      "Gradient Descent(42/249): loss=[30.8174046], w0=43.0, w1=0.5152491989919588\n",
      "Gradient Descent(43/249): loss=[29.81726102], w0=44.0, w1=0.5272317385033997\n",
      "Gradient Descent(44/249): loss=[28.81711743], w0=45.0, w1=0.5392142780148406\n",
      "Gradient Descent(45/249): loss=[27.81697385], w0=46.0, w1=0.5511968175262815\n",
      "Gradient Descent(46/249): loss=[26.81683027], w0=47.0, w1=0.5631793570377224\n",
      "Gradient Descent(47/249): loss=[25.81668669], w0=48.0, w1=0.5751618965491633\n",
      "Gradient Descent(48/249): loss=[24.83107844], w0=48.98, w1=0.619313933589209\n",
      "Gradient Descent(49/249): loss=[23.88473475], w0=49.94, w1=0.6946029712884897\n",
      "Gradient Descent(50/249): loss=[22.97774684], w0=50.86, w1=0.8280221824646078\n",
      "Gradient Descent(51/249): loss=[22.11354616], w0=51.78, w1=0.9614413936407259\n",
      "Gradient Descent(52/249): loss=[21.25659476], w0=52.68, w1=1.1202557997930296\n",
      "Gradient Descent(53/249): loss=[20.42137275], w0=53.58, w1=1.2790702059453332\n",
      "Gradient Descent(54/249): loss=[19.59696159], w0=54.44, w1=1.487930904228467\n",
      "Gradient Descent(55/249): loss=[18.83145831], w0=55.239999999999995, w1=1.7637366608269915\n",
      "Gradient Descent(56/249): loss=[18.11538949], w0=56.03999999999999, w1=2.039542417425516\n",
      "Gradient Descent(57/249): loss=[17.4110587], w0=56.79999999999999, w1=2.356423009492379\n",
      "Gradient Descent(58/249): loss=[16.74822831], w0=57.49999999999999, w1=2.71722013253828\n",
      "Gradient Descent(59/249): loss=[16.12805375], w0=58.199999999999996, w1=3.0780172555841814\n",
      "Gradient Descent(60/249): loss=[15.51980999], w0=58.85999999999999, w1=3.470148358413804\n",
      "Gradient Descent(61/249): loss=[14.94610875], w0=59.459999999999994, w1=3.90110721257904\n",
      "Gradient Descent(62/249): loss=[14.40810902], w0=60.019999999999996, w1=4.362620558818236\n",
      "Gradient Descent(63/249): loss=[13.89191468], w0=60.559999999999995, w1=4.824526910656732\n",
      "Gradient Descent(64/249): loss=[13.3869572], w0=61.099999999999994, w1=5.286433262495228\n",
      "Gradient Descent(65/249): loss=[12.88323418], w0=61.66, w1=5.7204311149353835\n",
      "Gradient Descent(66/249): loss=[12.38659828], w0=62.199999999999996, w1=6.167005974652881\n",
      "Gradient Descent(67/249): loss=[11.89984447], w0=62.72, w1=6.628009912422502\n",
      "Gradient Descent(68/249): loss=[11.4178887], w0=63.22, w1=7.095221193168687\n",
      "Gradient Descent(69/249): loss=[10.95480015], w0=63.699999999999996, w1=7.572294676421301\n",
      "Gradient Descent(70/249): loss=[10.49680104], w0=64.17999999999999, w1=8.049368159673914\n",
      "Gradient Descent(71/249): loss=[10.03880193], w0=64.66, w1=8.526441642926528\n",
      "Gradient Descent(72/249): loss=[9.58525887], w0=65.16, w1=8.964830643492478\n",
      "Gradient Descent(73/249): loss=[9.14578096], w0=65.64, w1=9.41827555936611\n",
      "Gradient Descent(74/249): loss=[8.71021308], w0=66.1, w1=9.87601923742406\n",
      "Gradient Descent(75/249): loss=[8.29002203], w0=66.58, w1=10.311356266109458\n",
      "Gradient Descent(76/249): loss=[7.87701616], w0=67.03999999999999, w1=10.751889404611289\n",
      "Gradient Descent(77/249): loss=[7.47726646], w0=67.46, w1=11.178568297399835\n",
      "Gradient Descent(78/249): loss=[7.11881158], w0=67.88, w1=11.605247190188381\n",
      "Gradient Descent(79/249): loss=[6.76353549], w0=68.28, w1=12.043224277032442\n",
      "Gradient Descent(80/249): loss=[6.42231202], w0=68.64, w1=12.46400441032125\n",
      "Gradient Descent(81/249): loss=[6.1156561], w0=69.0, w1=12.884784543610058\n",
      "Gradient Descent(82/249): loss=[5.80999949], w0=69.34, w1=13.318127202758443\n",
      "Gradient Descent(83/249): loss=[5.51408687], w0=69.7, w1=13.710916930459424\n",
      "Gradient Descent(84/249): loss=[5.23143993], w0=70.04, w1=14.108968394683787\n",
      "Gradient Descent(85/249): loss=[4.97351141], w0=70.36, w1=14.473638978893364\n",
      "Gradient Descent(86/249): loss=[4.74571308], w0=70.64, w1=14.808704933024712\n",
      "Gradient Descent(87/249): loss=[4.56411246], w0=70.9, w1=15.10963549181022\n",
      "Gradient Descent(88/249): loss=[4.41711465], w0=71.06, w1=15.31536641332248\n",
      "Gradient Descent(89/249): loss=[4.36293268], w0=71.18, w1=15.461504579988942\n",
      "Gradient Descent(90/249): loss=[4.33035725], w0=71.32000000000001, w1=15.51411009684392\n",
      "Gradient Descent(91/249): loss=[4.31044156], w0=71.4, w1=15.614216213527124\n",
      "Gradient Descent(92/249): loss=[4.29528382], w0=71.5, w1=15.682152832681723\n",
      "Gradient Descent(93/249): loss=[4.28066843], w0=71.6, w1=15.750089451836322\n",
      "Gradient Descent(94/249): loss=[4.2685867], w0=71.67999999999999, w1=15.79671219517414\n",
      "Gradient Descent(95/249): loss=[4.26001302], w0=71.75999999999999, w1=15.843334938511957\n",
      "Gradient Descent(96/249): loss=[4.25201457], w0=71.8, w1=15.834705833776903\n",
      "Gradient Descent(97/249): loss=[4.25034011], w0=71.84, w1=15.82607672904185\n",
      "Gradient Descent(98/249): loss=[4.24938671], w0=71.86, w1=15.8496171218354\n",
      "Gradient Descent(99/249): loss=[4.24843256], w0=71.88, w1=15.873157514628952\n",
      "Gradient Descent(100/249): loss=[4.24747841], w0=71.89999999999999, w1=15.896697907422503\n",
      "Gradient Descent(101/249): loss=[4.24687505], w0=71.94, w1=15.88806880268745\n",
      "Gradient Descent(102/249): loss=[4.24592739], w0=71.96, w1=15.911609195481\n",
      "Gradient Descent(103/249): loss=[4.24497324], w0=71.97999999999999, w1=15.935149588274552\n",
      "Gradient Descent(104/249): loss=[4.24401909], w0=71.99999999999999, w1=15.958689981068103\n",
      "Gradient Descent(105/249): loss=[4.24340998], w0=72.03999999999999, w1=15.95006087633305\n",
      "Gradient Descent(106/249): loss=[4.24246808], w0=72.05999999999999, w1=15.9736012691266\n",
      "Gradient Descent(107/249): loss=[4.24151393], w0=72.07999999999998, w1=15.997141661920152\n",
      "Gradient Descent(108/249): loss=[4.24055978], w0=72.09999999999998, w1=16.020682054713703\n",
      "Gradient Descent(109/249): loss=[4.23994492], w0=72.13999999999999, w1=16.01205294997865\n",
      "Gradient Descent(110/249): loss=[4.23900876], w0=72.15999999999998, w1=16.035593342772202\n",
      "Gradient Descent(111/249): loss=[4.23805461], w0=72.17999999999998, w1=16.059133735565755\n",
      "Gradient Descent(112/249): loss=[4.23710046], w0=72.19999999999997, w1=16.082674128359308\n",
      "Gradient Descent(113/249): loss=[4.23647986], w0=72.23999999999998, w1=16.074045023624254\n",
      "Gradient Descent(114/249): loss=[4.23554944], w0=72.25999999999998, w1=16.097585416417807\n",
      "Gradient Descent(115/249): loss=[4.23459529], w0=72.27999999999997, w1=16.12112580921136\n",
      "Gradient Descent(116/249): loss=[4.23364114], w0=72.29999999999997, w1=16.144666202004913\n",
      "Gradient Descent(117/249): loss=[4.23341459], w0=72.31999999999996, w1=16.129490763204526\n",
      "Gradient Descent(118/249): loss=[4.23334468], w0=72.31999999999996, w1=16.14648482193274\n",
      "Gradient Descent(119/249): loss=[4.23305588], w0=72.31999999999996, w1=16.163478880660957\n",
      "Gradient Descent(120/249): loss=[4.23330008], w0=72.33999999999996, w1=16.14830344186057\n",
      "Gradient Descent(121/249): loss=[4.23302497], w0=72.33999999999996, w1=16.165297500588785\n",
      "Gradient Descent(122/249): loss=[4.23292768], w0=72.35999999999996, w1=16.150122061788398\n",
      "Gradient Descent(123/249): loss=[4.233017], w0=72.33999999999996, w1=16.19825312117585\n",
      "Gradient Descent(124/249): loss=[4.23342779], w0=72.35999999999996, w1=16.183077682375462\n",
      "Gradient Descent(125/249): loss=[4.2327975], w0=72.37999999999995, w1=16.167902243575075\n",
      "Gradient Descent(126/249): loss=[4.23269191], w0=72.37999999999995, w1=16.18489630230329\n",
      "Gradient Descent(127/249): loss=[4.2324251], w0=72.39999999999995, w1=16.169720863502903\n",
      "Gradient Descent(128/249): loss=[4.23287369], w0=72.37999999999995, w1=16.217851922890354\n",
      "Gradient Descent(129/249): loss=[4.23292521], w0=72.39999999999995, w1=16.202676484089967\n",
      "Gradient Descent(130/249): loss=[4.23229492], w0=72.41999999999994, w1=16.18750104528958\n",
      "Gradient Descent(131/249): loss=[4.23241791], w0=72.39999999999995, w1=16.23563210467703\n",
      "Gradient Descent(132/249): loss=[4.23288427], w0=72.39999999999995, w1=16.202916254065855\n",
      "Gradient Descent(133/249): loss=[4.23229856], w0=72.41999999999994, w1=16.187740815265467\n",
      "Gradient Descent(134/249): loss=[4.23240637], w0=72.39999999999995, w1=16.23587187465292\n",
      "Gradient Descent(135/249): loss=[4.23289212], w0=72.39999999999995, w1=16.203156024041743\n",
      "Gradient Descent(136/249): loss=[4.2323022], w0=72.41999999999994, w1=16.187980585241355\n",
      "Gradient Descent(137/249): loss=[4.23239483], w0=72.39999999999995, w1=16.236111644628807\n",
      "Gradient Descent(138/249): loss=[4.23289996], w0=72.39999999999995, w1=16.20339579401763\n",
      "Gradient Descent(139/249): loss=[4.23230583], w0=72.41999999999994, w1=16.188220355217243\n",
      "Gradient Descent(140/249): loss=[4.23238329], w0=72.39999999999995, w1=16.236351414604695\n",
      "Gradient Descent(141/249): loss=[4.2329078], w0=72.39999999999995, w1=16.20363556399352\n",
      "Gradient Descent(142/249): loss=[4.23230947], w0=72.41999999999994, w1=16.18846012519313\n",
      "Gradient Descent(143/249): loss=[4.23237175], w0=72.39999999999995, w1=16.236591184580583\n",
      "Gradient Descent(144/249): loss=[4.23291565], w0=72.39999999999995, w1=16.203875333969407\n",
      "Gradient Descent(145/249): loss=[4.23231311], w0=72.41999999999994, w1=16.18869989516902\n",
      "Gradient Descent(146/249): loss=[4.23236021], w0=72.39999999999995, w1=16.23683095455647\n",
      "Gradient Descent(147/249): loss=[4.23292349], w0=72.39999999999995, w1=16.204115103945295\n",
      "Gradient Descent(148/249): loss=[4.23231675], w0=72.41999999999994, w1=16.188939665144908\n",
      "Gradient Descent(149/249): loss=[4.23234867], w0=72.39999999999995, w1=16.23707072453236\n",
      "Gradient Descent(150/249): loss=[4.23293134], w0=72.39999999999995, w1=16.204354873921183\n",
      "Gradient Descent(151/249): loss=[4.23232039], w0=72.41999999999994, w1=16.189179435120796\n",
      "Gradient Descent(152/249): loss=[4.23233713], w0=72.39999999999995, w1=16.237310494508247\n",
      "Gradient Descent(153/249): loss=[4.23293918], w0=72.39999999999995, w1=16.20459464389707\n",
      "Gradient Descent(154/249): loss=[4.23232403], w0=72.41999999999994, w1=16.189419205096684\n",
      "Gradient Descent(155/249): loss=[4.23232625], w0=72.41999999999994, w1=16.2064132638249\n",
      "Gradient Descent(156/249): loss=[4.23203745], w0=72.41999999999994, w1=16.223407322553115\n",
      "Gradient Descent(157/249): loss=[4.23248433], w0=72.41999999999994, w1=16.19069147194194\n",
      "Gradient Descent(158/249): loss=[4.23230463], w0=72.41999999999994, w1=16.207685530670155\n",
      "Gradient Descent(159/249): loss=[4.23201583], w0=72.41999999999994, w1=16.22467958939837\n",
      "Gradient Descent(160/249): loss=[4.23252595], w0=72.41999999999994, w1=16.191963738787194\n",
      "Gradient Descent(161/249): loss=[4.23228301], w0=72.41999999999994, w1=16.20895779751541\n",
      "Gradient Descent(162/249): loss=[4.23201557], w0=72.39999999999995, w1=16.208411444432837\n",
      "Gradient Descent(163/249): loss=[4.23238195], w0=72.41999999999994, w1=16.19323600563245\n",
      "Gradient Descent(164/249): loss=[4.23226139], w0=72.41999999999994, w1=16.210230064360665\n",
      "Gradient Descent(165/249): loss=[4.23205322], w0=72.41999999999994, w1=16.17751421374949\n",
      "Gradient Descent(166/249): loss=[4.23289859], w0=72.39999999999995, w1=16.22564527313694\n",
      "Gradient Descent(167/249): loss=[4.23264348], w0=72.41999999999994, w1=16.210469834336553\n",
      "Gradient Descent(168/249): loss=[4.23206107], w0=72.41999999999994, w1=16.177753983725378\n",
      "Gradient Descent(169/249): loss=[4.23288705], w0=72.39999999999995, w1=16.22588504311283\n",
      "Gradient Descent(170/249): loss=[4.23264712], w0=72.41999999999994, w1=16.21070960431244\n",
      "Gradient Descent(171/249): loss=[4.23206891], w0=72.41999999999994, w1=16.177993753701266\n",
      "Gradient Descent(172/249): loss=[4.23287551], w0=72.39999999999995, w1=16.226124813088717\n",
      "Gradient Descent(173/249): loss=[4.23265076], w0=72.41999999999994, w1=16.21094937428833\n",
      "Gradient Descent(174/249): loss=[4.23207676], w0=72.41999999999994, w1=16.178233523677154\n",
      "Gradient Descent(175/249): loss=[4.23286397], w0=72.39999999999995, w1=16.226364583064605\n",
      "Gradient Descent(176/249): loss=[4.2326544], w0=72.41999999999994, w1=16.211189144264218\n",
      "Gradient Descent(177/249): loss=[4.2320846], w0=72.41999999999994, w1=16.17847329365304\n",
      "Gradient Descent(178/249): loss=[4.23285243], w0=72.39999999999995, w1=16.226604353040493\n",
      "Gradient Descent(179/249): loss=[4.23265804], w0=72.41999999999994, w1=16.211428914240106\n",
      "Gradient Descent(180/249): loss=[4.23209244], w0=72.41999999999994, w1=16.17871306362893\n",
      "Gradient Descent(181/249): loss=[4.23284089], w0=72.39999999999995, w1=16.22684412301638\n",
      "Gradient Descent(182/249): loss=[4.23266167], w0=72.41999999999994, w1=16.211668684215994\n",
      "Gradient Descent(183/249): loss=[4.23210029], w0=72.41999999999994, w1=16.178952833604818\n",
      "Gradient Descent(184/249): loss=[4.23282934], w0=72.39999999999995, w1=16.22708389299227\n",
      "Gradient Descent(185/249): loss=[4.23266531], w0=72.41999999999994, w1=16.21190845419188\n",
      "Gradient Descent(186/249): loss=[4.23210813], w0=72.41999999999994, w1=16.179192603580706\n",
      "Gradient Descent(187/249): loss=[4.2328178], w0=72.39999999999995, w1=16.227323662968157\n",
      "Gradient Descent(188/249): loss=[4.23266895], w0=72.41999999999994, w1=16.21214822416777\n",
      "Gradient Descent(189/249): loss=[4.23211598], w0=72.41999999999994, w1=16.179432373556594\n",
      "Gradient Descent(190/249): loss=[4.23280626], w0=72.39999999999995, w1=16.227563432944045\n",
      "Gradient Descent(191/249): loss=[4.23267259], w0=72.41999999999994, w1=16.212387994143658\n",
      "Gradient Descent(192/249): loss=[4.23212382], w0=72.41999999999994, w1=16.179672143532482\n",
      "Gradient Descent(193/249): loss=[4.23279472], w0=72.39999999999995, w1=16.227803202919933\n",
      "Gradient Descent(194/249): loss=[4.23267623], w0=72.41999999999994, w1=16.212627764119546\n",
      "Gradient Descent(195/249): loss=[4.23213167], w0=72.41999999999994, w1=16.17991191350837\n",
      "Gradient Descent(196/249): loss=[4.23278318], w0=72.39999999999995, w1=16.22804297289582\n",
      "Gradient Descent(197/249): loss=[4.23267987], w0=72.41999999999994, w1=16.212867534095434\n",
      "Gradient Descent(198/249): loss=[4.23213951], w0=72.41999999999994, w1=16.180151683484258\n",
      "Gradient Descent(199/249): loss=[4.23277164], w0=72.39999999999995, w1=16.22828274287171\n",
      "Gradient Descent(200/249): loss=[4.23268351], w0=72.41999999999994, w1=16.213107304071322\n",
      "Gradient Descent(201/249): loss=[4.23214735], w0=72.41999999999994, w1=16.180391453460146\n",
      "Gradient Descent(202/249): loss=[4.2327601], w0=72.39999999999995, w1=16.228522512847597\n",
      "Gradient Descent(203/249): loss=[4.23268714], w0=72.41999999999994, w1=16.21334707404721\n",
      "Gradient Descent(204/249): loss=[4.2321552], w0=72.41999999999994, w1=16.180631223436034\n",
      "Gradient Descent(205/249): loss=[4.23274856], w0=72.39999999999995, w1=16.228762282823485\n",
      "Gradient Descent(206/249): loss=[4.23269078], w0=72.41999999999994, w1=16.213586844023098\n",
      "Gradient Descent(207/249): loss=[4.23216304], w0=72.41999999999994, w1=16.180870993411922\n",
      "Gradient Descent(208/249): loss=[4.23273702], w0=72.39999999999995, w1=16.229002052799373\n",
      "Gradient Descent(209/249): loss=[4.23269442], w0=72.41999999999994, w1=16.213826613998986\n",
      "Gradient Descent(210/249): loss=[4.23217089], w0=72.41999999999994, w1=16.18111076338781\n",
      "Gradient Descent(211/249): loss=[4.23272548], w0=72.39999999999995, w1=16.22924182277526\n",
      "Gradient Descent(212/249): loss=[4.23269806], w0=72.41999999999994, w1=16.214066383974874\n",
      "Gradient Descent(213/249): loss=[4.23217873], w0=72.41999999999994, w1=16.181350533363698\n",
      "Gradient Descent(214/249): loss=[4.23271394], w0=72.39999999999995, w1=16.22948159275115\n",
      "Gradient Descent(215/249): loss=[4.2327017], w0=72.41999999999994, w1=16.214306153950762\n",
      "Gradient Descent(216/249): loss=[4.23218658], w0=72.41999999999994, w1=16.181590303339586\n",
      "Gradient Descent(217/249): loss=[4.2327024], w0=72.39999999999995, w1=16.229721362727037\n",
      "Gradient Descent(218/249): loss=[4.23270534], w0=72.41999999999994, w1=16.21454592392665\n",
      "Gradient Descent(219/249): loss=[4.23219442], w0=72.41999999999994, w1=16.181830073315474\n",
      "Gradient Descent(220/249): loss=[4.23269086], w0=72.39999999999995, w1=16.229961132702925\n",
      "Gradient Descent(221/249): loss=[4.23270898], w0=72.41999999999994, w1=16.214785693902538\n",
      "Gradient Descent(222/249): loss=[4.23220226], w0=72.41999999999994, w1=16.182069843291362\n",
      "Gradient Descent(223/249): loss=[4.23267932], w0=72.39999999999995, w1=16.230200902678813\n",
      "Gradient Descent(224/249): loss=[4.23271261], w0=72.41999999999994, w1=16.215025463878426\n",
      "Gradient Descent(225/249): loss=[4.23221011], w0=72.41999999999994, w1=16.18230961326725\n",
      "Gradient Descent(226/249): loss=[4.23266778], w0=72.39999999999995, w1=16.2304406726547\n",
      "Gradient Descent(227/249): loss=[4.23271625], w0=72.41999999999994, w1=16.215265233854314\n",
      "Gradient Descent(228/249): loss=[4.23221795], w0=72.41999999999994, w1=16.18254938324314\n",
      "Gradient Descent(229/249): loss=[4.23265624], w0=72.39999999999995, w1=16.23068044263059\n",
      "Gradient Descent(230/249): loss=[4.23272227], w0=72.39999999999995, w1=16.197964592019414\n",
      "Gradient Descent(231/249): loss=[4.23222341], w0=72.41999999999994, w1=16.182789153219026\n",
      "Gradient Descent(232/249): loss=[4.2326447], w0=72.39999999999995, w1=16.230920212606478\n",
      "Gradient Descent(233/249): loss=[4.23273012], w0=72.39999999999995, w1=16.198204361995302\n",
      "Gradient Descent(234/249): loss=[4.23222705], w0=72.41999999999994, w1=16.183028923194914\n",
      "Gradient Descent(235/249): loss=[4.23263316], w0=72.39999999999995, w1=16.231159982582366\n",
      "Gradient Descent(236/249): loss=[4.23273796], w0=72.39999999999995, w1=16.19844413197119\n",
      "Gradient Descent(237/249): loss=[4.23223069], w0=72.41999999999994, w1=16.183268693170803\n",
      "Gradient Descent(238/249): loss=[4.23262162], w0=72.39999999999995, w1=16.231399752558254\n",
      "Gradient Descent(239/249): loss=[4.23274581], w0=72.39999999999995, w1=16.198683901947078\n",
      "Gradient Descent(240/249): loss=[4.23223433], w0=72.41999999999994, w1=16.18350846314669\n",
      "Gradient Descent(241/249): loss=[4.23261008], w0=72.39999999999995, w1=16.23163952253414\n",
      "Gradient Descent(242/249): loss=[4.23275365], w0=72.39999999999995, w1=16.198923671922966\n",
      "Gradient Descent(243/249): loss=[4.23223797], w0=72.41999999999994, w1=16.18374823312258\n",
      "Gradient Descent(244/249): loss=[4.23259854], w0=72.39999999999995, w1=16.23187929251003\n",
      "Gradient Descent(245/249): loss=[4.2327615], w0=72.39999999999995, w1=16.199163441898854\n",
      "Gradient Descent(246/249): loss=[4.23224161], w0=72.41999999999994, w1=16.183988003098467\n",
      "Gradient Descent(247/249): loss=[4.232587], w0=72.39999999999995, w1=16.232119062485918\n",
      "Gradient Descent(248/249): loss=[4.23276934], w0=72.39999999999995, w1=16.199403211874742\n",
      "Gradient Descent(249/249): loss=[4.23224525], w0=72.41999999999994, w1=16.184227773074355\n",
      "SGD: execution time=0.074 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 250\n",
    "gamma = 1\n",
    "batch_size = 100\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "ssgd_losses, ssgd_ws = stochastic_sub_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b0bc42b7f54089af2238e4b1128bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=101, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        ssgd_losses, ssgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
