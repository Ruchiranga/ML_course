{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `compute_loss` function below:\n",
    "<a id='compute_loss'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    y = np.array([y]).T.reshape([len(y), 1])\n",
    "    w = np.array([w]).T.reshape([len(w), 1])\n",
    "    \n",
    "    e = y - np.matmul(tx,w) \n",
    "    \n",
    "    return (1/y.shape[0]) * (e.T.dot(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Grid Search\n",
    "\n",
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    \n",
    "    for i in range(0, w0.shape[0]):\n",
    "        for j in range(0, w1.shape[0]):\n",
    "            losses[i, j] = compute_loss_mse(y, tx, np.array([[w0[i]], [w1[j]]]))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=37.58708203904648, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.117 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "\n",
    "    e = y - np.matmul(tx,w)\n",
    "    return (-1/y.shape[0]) * np.matmul(tx.T, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mse(y, tx, w)\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=[[5584.47342552]], w0=51.30574540147363, w1=9.435798704492253\n",
      "Gradient Descent(1/149): loss=[[530.60492422]], w0=66.69746902191572, w1=12.266538315839991\n",
      "Gradient Descent(2/149): loss=[[75.7567591]], w0=71.31498610804836, w1=13.115760199244326\n",
      "Gradient Descent(3/149): loss=[[34.82042424]], w0=72.70024123388814, w1=13.37052676426563\n",
      "Gradient Descent(4/149): loss=[[31.1361541]], w0=73.11581777164007, w1=13.446956733772023\n",
      "Gradient Descent(5/149): loss=[[30.80456979]], w0=73.24049073296565, w1=13.469885724623941\n",
      "Gradient Descent(6/149): loss=[[30.7747272]], w0=73.27789262136334, w1=13.476764421879516\n",
      "Gradient Descent(7/149): loss=[[30.77204137]], w0=73.28911318788263, w1=13.478828031056189\n",
      "Gradient Descent(8/149): loss=[[30.77179964]], w0=73.29247935783842, w1=13.47944711380919\n",
      "Gradient Descent(9/149): loss=[[30.77177789]], w0=73.29348920882516, w1=13.47963283863509\n",
      "Gradient Descent(10/149): loss=[[30.77177593]], w0=73.29379216412119, w1=13.479688556082861\n",
      "Gradient Descent(11/149): loss=[[30.77177576]], w0=73.29388305071, w1=13.479705271317192\n",
      "Gradient Descent(12/149): loss=[[30.77177574]], w0=73.29391031668663, w1=13.479710285887492\n",
      "Gradient Descent(13/149): loss=[[30.77177574]], w0=73.29391849647962, w1=13.479711790258582\n",
      "Gradient Descent(14/149): loss=[[30.77177574]], w0=73.29392095041752, w1=13.479712241569908\n",
      "Gradient Descent(15/149): loss=[[30.77177574]], w0=73.29392168659889, w1=13.479712376963306\n",
      "Gradient Descent(16/149): loss=[[30.77177574]], w0=73.2939219074533, w1=13.479712417581325\n",
      "Gradient Descent(17/149): loss=[[30.77177574]], w0=73.29392197370963, w1=13.479712429766732\n",
      "Gradient Descent(18/149): loss=[[30.77177574]], w0=73.29392199358652, w1=13.479712433422353\n",
      "Gradient Descent(19/149): loss=[[30.77177574]], w0=73.2939219995496, w1=13.47971243451904\n",
      "Gradient Descent(20/149): loss=[[30.77177574]], w0=73.29392200133852, w1=13.479712434848047\n",
      "Gradient Descent(21/149): loss=[[30.77177574]], w0=73.29392200187519, w1=13.479712434946748\n",
      "Gradient Descent(22/149): loss=[[30.77177574]], w0=73.29392200203618, w1=13.479712434976358\n",
      "Gradient Descent(23/149): loss=[[30.77177574]], w0=73.29392200208449, w1=13.479712434985242\n",
      "Gradient Descent(24/149): loss=[[30.77177574]], w0=73.29392200209898, w1=13.479712434987906\n",
      "Gradient Descent(25/149): loss=[[30.77177574]], w0=73.29392200210333, w1=13.479712434988706\n",
      "Gradient Descent(26/149): loss=[[30.77177574]], w0=73.29392200210464, w1=13.479712434988945\n",
      "Gradient Descent(27/149): loss=[[30.77177574]], w0=73.29392200210502, w1=13.479712434989018\n",
      "Gradient Descent(28/149): loss=[[30.77177574]], w0=73.29392200210513, w1=13.47971243498904\n",
      "Gradient Descent(29/149): loss=[[30.77177574]], w0=73.29392200210518, w1=13.479712434989047\n",
      "Gradient Descent(30/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(31/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(32/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(33/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(34/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(35/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(36/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(37/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(38/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(39/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(40/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(41/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(42/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(43/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(44/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(45/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(46/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(47/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(48/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(49/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(50/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(51/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(52/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(53/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(54/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(55/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(56/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(57/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(58/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(59/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(60/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(61/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(62/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(63/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(64/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(65/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(66/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(67/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(68/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(69/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(70/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(71/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(72/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(73/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(74/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(75/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(76/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(77/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(78/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(79/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(80/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(81/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(82/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(83/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(84/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(85/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(86/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(87/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(88/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(89/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(90/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(91/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(92/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(93/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(94/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(95/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(96/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(97/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(98/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(99/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(100/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(101/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(102/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(103/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(104/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(105/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(106/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(107/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(108/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(109/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(110/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(111/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(112/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(113/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(114/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(115/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(116/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(117/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(118/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(119/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(120/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(121/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(122/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(123/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(124/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(125/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(126/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(127/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(128/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(129/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(130/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(131/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(132/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(133/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(134/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(135/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(136/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(137/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(138/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(139/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(140/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(141/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(142/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(143/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(144/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(145/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(146/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(147/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(148/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(149/149): loss=[[30.77177574]], w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent: execution time=0.092 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5ec080b9dc46879d5eab1586497951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=151, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    \n",
    "    return compute_gradient(y, tx, w)\n",
    "    \n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "      \n",
    "    initial_w = initial_w.reshape((2,1))\n",
    "    \n",
    "    gen = batch_iter(y, tx, batch_size)\n",
    "    \n",
    "    y = []\n",
    "    tx = []\n",
    "    for pair in gen:\n",
    "        y.append(pair[0])\n",
    "        tx.append(pair[1])\n",
    "        \n",
    "    y = np.asarray(y).T\n",
    "    tx = np.asarray(tx).reshape((10,2))\n",
    "    \n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mse(y, tx, [w[0], w[1]])\n",
    "        grad = compute_stoch_gradient(y, tx, [w[0], w[1]])\n",
    "\n",
    "        w = w - (gamma * grad)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=[[4888.1547836]], w0=[47.4916341], w1=[-12.1194549]\n",
      "Gradient Descent(1/149): loss=[[1126.46735269]], w0=[57.22878506], w1=[6.98936703]\n",
      "Gradient Descent(2/149): loss=[[292.64667979]], w0=[67.26141108], w1=[8.35136059]\n",
      "Gradient Descent(3/149): loss=[[88.8528165]], w0=[70.77807424], w1=[11.92386399]\n",
      "Gradient Descent(4/149): loss=[[36.45255589]], w0=[73.16260513], w1=[12.80976662]\n",
      "Gradient Descent(5/149): loss=[[22.6582]], w0=[74.20765921], w1=[13.59232931]\n",
      "Gradient Descent(6/149): loss=[[18.98903781]], w0=[74.81241157], w1=[13.8886279]\n",
      "Gradient Descent(7/149): loss=[[18.00872902]], w0=[75.10410686], w1=[14.07862025]\n",
      "Gradient Descent(8/149): loss=[[17.74631996]], w0=[75.26232242], w1=[14.16468884]\n",
      "Gradient Descent(9/149): loss=[[17.67602221]], w0=[75.34181811], w1=[14.21338262]\n",
      "Gradient Descent(10/149): loss=[[17.65718354]], w0=[75.38378855], w1=[14.23720401]\n",
      "Gradient Descent(11/149): loss=[[17.65213436]], w0=[75.40524497], w1=[14.25000406]\n",
      "Gradient Descent(12/149): loss=[[17.65078098]], w0=[75.41644553], w1=[14.25647418]\n",
      "Gradient Descent(13/149): loss=[[17.65041822]], w0=[75.42221359], w1=[14.25987673]\n",
      "Gradient Descent(14/149): loss=[[17.65032098]], w0=[75.42521029], w1=[14.26162062]\n",
      "Gradient Descent(15/149): loss=[[17.65029492]], w0=[75.42675831], w1=[14.26252946]\n",
      "Gradient Descent(16/149): loss=[[17.65028793]], w0=[75.42756094], w1=[14.26299799]\n",
      "Gradient Descent(17/149): loss=[[17.65028606]], w0=[75.4279761], w1=[14.26324124]\n",
      "Gradient Descent(18/149): loss=[[17.65028556]], w0=[75.42819117], w1=[14.26336695]\n",
      "Gradient Descent(19/149): loss=[[17.65028542]], w0=[75.42830248], w1=[14.26343211]\n",
      "Gradient Descent(20/149): loss=[[17.65028538]], w0=[75.42836012], w1=[14.26346582]\n",
      "Gradient Descent(21/149): loss=[[17.65028538]], w0=[75.42838996], w1=[14.26348328]\n",
      "Gradient Descent(22/149): loss=[[17.65028537]], w0=[75.42840541], w1=[14.26349232]\n",
      "Gradient Descent(23/149): loss=[[17.65028537]], w0=[75.42841341], w1=[14.263497]\n",
      "Gradient Descent(24/149): loss=[[17.65028537]], w0=[75.42841755], w1=[14.26349942]\n",
      "Gradient Descent(25/149): loss=[[17.65028537]], w0=[75.42841969], w1=[14.26350068]\n",
      "Gradient Descent(26/149): loss=[[17.65028537]], w0=[75.4284208], w1=[14.26350133]\n",
      "Gradient Descent(27/149): loss=[[17.65028537]], w0=[75.42842138], w1=[14.26350166]\n",
      "Gradient Descent(28/149): loss=[[17.65028537]], w0=[75.42842168], w1=[14.26350184]\n",
      "Gradient Descent(29/149): loss=[[17.65028537]], w0=[75.42842183], w1=[14.26350193]\n",
      "Gradient Descent(30/149): loss=[[17.65028537]], w0=[75.42842191], w1=[14.26350198]\n",
      "Gradient Descent(31/149): loss=[[17.65028537]], w0=[75.42842195], w1=[14.263502]\n",
      "Gradient Descent(32/149): loss=[[17.65028537]], w0=[75.42842197], w1=[14.26350201]\n",
      "Gradient Descent(33/149): loss=[[17.65028537]], w0=[75.42842198], w1=[14.26350202]\n",
      "Gradient Descent(34/149): loss=[[17.65028537]], w0=[75.42842199], w1=[14.26350202]\n",
      "Gradient Descent(35/149): loss=[[17.65028537]], w0=[75.42842199], w1=[14.26350202]\n",
      "Gradient Descent(36/149): loss=[[17.65028537]], w0=[75.42842199], w1=[14.26350202]\n",
      "Gradient Descent(37/149): loss=[[17.65028537]], w0=[75.42842199], w1=[14.26350203]\n",
      "Gradient Descent(38/149): loss=[[17.65028537]], w0=[75.42842199], w1=[14.26350203]\n",
      "Gradient Descent(39/149): loss=[[17.65028537]], w0=[75.42842199], w1=[14.26350203]\n",
      "Gradient Descent(40/149): loss=[[17.65028537]], w0=[75.42842199], w1=[14.26350203]\n",
      "Gradient Descent(41/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(42/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(43/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(44/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(45/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(46/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(47/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(48/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(49/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(50/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(51/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(52/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(53/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(54/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(55/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(56/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(57/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(58/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(59/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(60/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(61/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(62/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(63/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(64/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(65/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(66/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(67/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(68/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(69/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(70/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(71/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(72/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(73/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(74/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(75/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(76/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(77/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(78/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(79/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(80/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(81/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(82/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(83/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(84/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(85/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(86/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(87/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(88/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(89/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(90/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(91/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(92/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(93/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(94/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(95/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(96/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(97/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(98/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(99/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(100/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(101/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(102/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(103/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(104/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(105/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(106/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(107/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(108/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(109/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(110/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(111/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(112/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(113/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(114/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(115/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(116/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(117/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(118/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(119/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(120/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(121/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(122/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(123/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(124/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(125/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(126/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(127/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(128/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(129/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(130/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(131/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(132/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(133/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(134/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(135/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(136/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(137/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(138/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(139/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(140/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(141/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(142/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(143/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(144/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(145/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(146/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(147/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(148/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "Gradient Descent(149/149): loss=[[17.65028537]], w0=[75.428422], w1=[14.26350203]\n",
      "SGD: execution time=0.076 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.7\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadbd8cd4a4c4fe190d2cf590c16adea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=151, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Effect of Outliers and MAE Cost Function, and Subgradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Load and plot data containing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=[[5739.67022907]], w0=51.84746409844842, w1=7.7244264061924195\n",
      "Gradient Descent(1/149): loss=[[636.5642494]], w0=67.40170332798297, w1=10.041754328050114\n",
      "Gradient Descent(2/149): loss=[[177.28471123]], w0=72.06797509684336, w1=10.736952704607411\n",
      "Gradient Descent(3/149): loss=[[135.9495528]], w0=73.46785662750146, w1=10.945512217574597\n",
      "Gradient Descent(4/149): loss=[[132.22938854]], w0=73.88782108669889, w1=11.00808007146475\n",
      "Gradient Descent(5/149): loss=[[131.89457376]], w0=74.01381042445813, w1=11.026850427631798\n",
      "Gradient Descent(6/149): loss=[[131.86444042]], w0=74.0516072257859, w1=11.032481534481914\n",
      "Gradient Descent(7/149): loss=[[131.86172842]], w0=74.06294626618423, w1=11.034170866536945\n",
      "Gradient Descent(8/149): loss=[[131.86148434]], w0=74.06634797830372, w1=11.034677666153454\n",
      "Gradient Descent(9/149): loss=[[131.86146238]], w0=74.06736849193958, w1=11.034829706038408\n",
      "Gradient Descent(10/149): loss=[[131.8614604]], w0=74.06767464603033, w1=11.034875318003895\n",
      "Gradient Descent(11/149): loss=[[131.86146022]], w0=74.06776649225755, w1=11.034889001593541\n",
      "Gradient Descent(12/149): loss=[[131.86146021]], w0=74.06779404612573, w1=11.034893106670431\n",
      "Gradient Descent(13/149): loss=[[131.86146021]], w0=74.06780231228618, w1=11.034894338193501\n",
      "Gradient Descent(14/149): loss=[[131.86146021]], w0=74.06780479213431, w1=11.034894707650421\n",
      "Gradient Descent(15/149): loss=[[131.86146021]], w0=74.06780553608874, w1=11.034894818487496\n",
      "Gradient Descent(16/149): loss=[[131.86146021]], w0=74.06780575927507, w1=11.03489485173862\n",
      "Gradient Descent(17/149): loss=[[131.86146021]], w0=74.06780582623098, w1=11.034894861713957\n",
      "Gradient Descent(18/149): loss=[[131.86146021]], w0=74.06780584631775, w1=11.034894864706557\n",
      "Gradient Descent(19/149): loss=[[131.86146021]], w0=74.06780585234378, w1=11.034894865604338\n",
      "Gradient Descent(20/149): loss=[[131.86146021]], w0=74.06780585415159, w1=11.034894865873675\n",
      "Gradient Descent(21/149): loss=[[131.86146021]], w0=74.06780585469393, w1=11.034894865954474\n",
      "Gradient Descent(22/149): loss=[[131.86146021]], w0=74.06780585485663, w1=11.034894865978712\n",
      "Gradient Descent(23/149): loss=[[131.86146021]], w0=74.06780585490544, w1=11.034894865985985\n",
      "Gradient Descent(24/149): loss=[[131.86146021]], w0=74.0678058549201, w1=11.034894865988166\n",
      "Gradient Descent(25/149): loss=[[131.86146021]], w0=74.06780585492449, w1=11.034894865988822\n",
      "Gradient Descent(26/149): loss=[[131.86146021]], w0=74.06780585492581, w1=11.034894865989015\n",
      "Gradient Descent(27/149): loss=[[131.86146021]], w0=74.06780585492619, w1=11.034894865989076\n",
      "Gradient Descent(28/149): loss=[[131.86146021]], w0=74.06780585492632, w1=11.034894865989099\n",
      "Gradient Descent(29/149): loss=[[131.86146021]], w0=74.06780585492635, w1=11.0348948659891\n",
      "Gradient Descent(30/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(31/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(32/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(33/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(34/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(35/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(36/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(37/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(38/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(39/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(40/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(41/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(42/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(43/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(44/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(45/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(46/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(47/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(48/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(49/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(50/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(51/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(52/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(53/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(54/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(55/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(56/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(57/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(58/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(59/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(60/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(61/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(62/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(63/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(64/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(65/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(66/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(67/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(68/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(69/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(70/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(71/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(72/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(73/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(74/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(75/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(76/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(77/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(78/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(79/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(80/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(81/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(82/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(83/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(84/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(85/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(86/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(87/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(88/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(89/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(90/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(91/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(92/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(93/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(94/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(95/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(96/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(97/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(98/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(99/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(100/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(101/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(102/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(103/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(104/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(105/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(106/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(107/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(108/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(109/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(110/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(111/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(112/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(113/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(114/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(115/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(116/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(117/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(118/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(119/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(120/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(121/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(122/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(123/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(124/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(125/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(126/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(127/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(128/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(129/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(130/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(131/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(132/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(133/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(134/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(135/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(136/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(137/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(138/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(139/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(140/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(141/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(142/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(143/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(144/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(145/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(146/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(147/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(148/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(149/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "[74.06780585 11.03489487]\n",
      "[1.2  1.21 1.22 1.23 1.24 1.25 1.26 1.27 1.28 1.29 1.3  1.31 1.32 1.33\n",
      " 1.34 1.35 1.36 1.37 1.38 1.39 1.4  1.41 1.42 1.43 1.44 1.45 1.46 1.47\n",
      " 1.48 1.49 1.5  1.51 1.52 1.53 1.54 1.55 1.56 1.57 1.58 1.59 1.6  1.61\n",
      " 1.62 1.63 1.64 1.65 1.66 1.67 1.68 1.69 1.7  1.71 1.72 1.73 1.74 1.75\n",
      " 1.76 1.77 1.78 1.79 1.8  1.81 1.82 1.83 1.84 1.85 1.86 1.87 1.88 1.89\n",
      " 1.9  1.91 1.92 1.93 1.94 1.95 1.96 1.97 1.98 1.99]\n",
      "[ 30.39642599  31.34675742  32.29708884  33.24742026  34.19775168\n",
      "  35.1480831   36.09841452  37.04874594  37.99907736  38.94940878\n",
      "  39.89974021  40.85007163  41.80040305  42.75073447  43.70106589\n",
      "  44.65139731  45.60172873  46.55206015  47.50239157  48.45272299\n",
      "  49.40305442  50.35338584  51.30371726  52.25404868  53.2043801\n",
      "  54.15471152  55.10504294  56.05537436  57.00570578  57.95603721\n",
      "  58.90636863  59.85670005  60.80703147  61.75736289  62.70769431\n",
      "  63.65802573  64.60835715  65.55868857  66.50902     67.45935142\n",
      "  68.40968284  69.36001426  70.31034568  71.2606771   72.21100852\n",
      "  73.16133994  74.11167136  75.06200279  76.01233421  76.96266563\n",
      "  77.91299705  78.86332847  79.81365989  80.76399131  81.71432273\n",
      "  82.66465415  83.61498558  84.565317    85.51564842  86.46597984\n",
      "  87.41631126  88.36664268  89.3169741   90.26730552  91.21763694\n",
      "  92.16796837  93.11829979  94.06863121  95.01896263  95.96929405\n",
      "  96.91962547  97.86995689  98.82028831  99.77061973 100.72095115\n",
      " 101.67128258 102.621614   103.57194542 104.52227684 105.47260826]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x112b58a90>]"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "plt.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.7\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "w = sgd_ws[-1]\n",
    "print(w)\n",
    "def prediction(w0, w1, mean_x, std_x):\n",
    "    \"\"\"Get the regression line from the model.\"\"\"\n",
    "    x = np.arange(1.2, 2, 0.01)\n",
    "    x_normalized = (x - mean_x) / std_x\n",
    "    return x, w0 + w1 * x_normalized\n",
    "\n",
    "pred_x, pred_y = prediction(\n",
    "        w[0], w[1],\n",
    "        mean_x, std_x)\n",
    "print(pred_x)\n",
    "print(pred_y)\n",
    "plt.plot(pred_x, pred_y, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function `compute_loss(y, tx, w)` for the Mean Absolute Error cost function [here](#compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "    y = np.array([y]).T\n",
    "    w = np.array([w]).T\n",
    "    \n",
    "    e = y - np.matmul(tx,w)  \n",
    "    \n",
    "    return (1/y.shape[0]) * (np.sum(np.abs(e), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sub_gradient(y, tx, w):\n",
    "    e = y - np.matmul(tx,w)\n",
    "\n",
    "    if 0 in np.sign(e):\n",
    "        print(colored('Non-deferentiable point found!', 'red'))\n",
    "    \n",
    "    return (1/y.shape[0]) * (np.sign(e).T).dot(-tx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        grad = compute_sub_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=[74.06780585], w0=0.7, w1=6.109524327590712e-16\n",
      "Gradient Descent(1/149): loss=[73.36780585], w0=1.4, w1=1.2219048655181425e-15\n",
      "Gradient Descent(2/149): loss=[72.66780585], w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "Gradient Descent(3/149): loss=[71.96780585], w0=2.8, w1=2.443809731036285e-15\n",
      "Gradient Descent(4/149): loss=[71.26780585], w0=3.5, w1=3.054762163795356e-15\n",
      "Gradient Descent(5/149): loss=[70.56780585], w0=4.2, w1=3.665714596554428e-15\n",
      "Gradient Descent(6/149): loss=[69.86780585], w0=4.9, w1=4.276667029313499e-15\n",
      "Gradient Descent(7/149): loss=[69.16780585], w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "Gradient Descent(8/149): loss=[68.46780585], w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "Gradient Descent(9/149): loss=[67.76780585], w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "Gradient Descent(10/149): loss=[67.06780585], w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "Gradient Descent(11/149): loss=[66.36780585], w0=8.4, w1=7.331429193108857e-15\n",
      "Gradient Descent(12/149): loss=[65.66780585], w0=9.1, w1=7.942381625867928e-15\n",
      "Gradient Descent(13/149): loss=[64.96780585], w0=9.799999999999999, w1=8.553334058627e-15\n",
      "Gradient Descent(14/149): loss=[64.26780585], w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "Gradient Descent(15/149): loss=[63.56780585], w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "Gradient Descent(16/149): loss=[62.86780585], w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "Gradient Descent(17/149): loss=[62.16780585], w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "Gradient Descent(18/149): loss=[61.46780585], w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "Gradient Descent(19/149): loss=[60.76780585], w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "Gradient Descent(20/149): loss=[60.06780585], w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "Gradient Descent(21/149): loss=[59.36780585], w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "Gradient Descent(22/149): loss=[58.66780585], w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "Gradient Descent(23/149): loss=[57.96780585], w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "Gradient Descent(24/149): loss=[57.26780585], w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "Gradient Descent(25/149): loss=[56.56780585], w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "Gradient Descent(26/149): loss=[55.86780585], w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "Gradient Descent(27/149): loss=[55.16780585], w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "Gradient Descent(28/149): loss=[54.46780585], w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "Gradient Descent(29/149): loss=[53.76780585], w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "Gradient Descent(30/149): loss=[53.06780585], w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "Gradient Descent(31/149): loss=[52.36780585], w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "Gradient Descent(32/149): loss=[51.66780585], w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "Gradient Descent(33/149): loss=[50.96780585], w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "Gradient Descent(34/149): loss=[50.26780585], w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "Gradient Descent(35/149): loss=[49.56780585], w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "Gradient Descent(36/149): loss=[48.86780585], w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "Gradient Descent(37/149): loss=[48.16780585], w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "Gradient Descent(38/149): loss=[47.46780585], w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "Gradient Descent(39/149): loss=[46.76780585], w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "Gradient Descent(40/149): loss=[46.06780585], w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "Gradient Descent(41/149): loss=[45.36780585], w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "Gradient Descent(42/149): loss=[44.66780585], w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "Gradient Descent(43/149): loss=[43.96780585], w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "Gradient Descent(44/149): loss=[43.26780585], w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "Gradient Descent(45/149): loss=[42.56780585], w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "Gradient Descent(46/149): loss=[41.86780585], w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "Gradient Descent(47/149): loss=[41.16780585], w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "Gradient Descent(48/149): loss=[40.46780585], w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "Gradient Descent(49/149): loss=[39.76780585], w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "Gradient Descent(50/149): loss=[39.06780585], w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "Gradient Descent(51/149): loss=[38.36780585], w0=36.4, w1=3.176952650347169e-14\n",
      "Gradient Descent(52/149): loss=[37.66780585], w0=37.1, w1=3.238047893623076e-14\n",
      "Gradient Descent(53/149): loss=[36.96780585], w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "Gradient Descent(54/149): loss=[36.26780585], w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "Gradient Descent(55/149): loss=[35.56780585], w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "Gradient Descent(56/149): loss=[34.86780585], w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "Gradient Descent(57/149): loss=[34.16780585], w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "Gradient Descent(58/149): loss=[33.46780585], w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "Gradient Descent(59/149): loss=[32.76780585], w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "Gradient Descent(60/149): loss=[32.06780585], w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "Gradient Descent(61/149): loss=[31.36780585], w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "Gradient Descent(62/149): loss=[30.66780585], w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "Gradient Descent(63/149): loss=[29.96780585], w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "Gradient Descent(64/149): loss=[29.26780585], w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "Gradient Descent(65/149): loss=[28.56780585], w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "Gradient Descent(66/149): loss=[27.86780585], w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "Gradient Descent(67/149): loss=[27.17327021], w0=47.59306930693074, w1=0.011147845678271063\n",
      "Gradient Descent(68/149): loss=[26.49045156], w0=48.279207920792125, w1=0.03308574108989941\n",
      "Gradient Descent(69/149): loss=[25.81721232], w0=48.96534653465351, w1=0.05502363650152776\n",
      "Gradient Descent(70/149): loss=[25.15503943], w0=49.63069306930698, w1=0.10538326388307814\n",
      "Gradient Descent(71/149): loss=[24.52410341], w0=50.28910891089114, w1=0.16746568532793435\n",
      "Gradient Descent(72/149): loss=[23.89929535], w0=50.947524752475296, w1=0.22954810677279056\n",
      "Gradient Descent(73/149): loss=[23.28439293], w0=51.59207920792084, w1=0.31242512932747524\n",
      "Gradient Descent(74/149): loss=[22.68687644], w0=52.22277227722777, w1=0.4119501328839991\n",
      "Gradient Descent(75/149): loss=[22.10626757], w0=52.84653465346539, w1=0.5208167847923756\n",
      "Gradient Descent(76/149): loss=[21.53781883], w0=53.4564356435644, w1=0.6457900912635992\n",
      "Gradient Descent(77/149): loss=[20.98633987], w0=54.0594059405941, w1=0.7796904498577214\n",
      "Gradient Descent(78/149): loss=[20.44556094], w0=54.655445544554496, w1=0.9197570104995693\n",
      "Gradient Descent(79/149): loss=[19.91191016], w0=55.24455445544559, w1=1.0670920297849913\n",
      "Gradient Descent(80/149): loss=[19.38964409], w0=55.819801980198065, w1=1.2261255948210765\n",
      "Gradient Descent(81/149): loss=[18.88798906], w0=56.36732673267331, w1=1.410709342622213\n",
      "Gradient Descent(82/149): loss=[18.4159605], w0=56.900990099009945, w1=1.605853732220269\n",
      "Gradient Descent(83/149): loss=[17.95489854], w0=57.42772277227727, w1=1.808762802293962\n",
      "Gradient Descent(84/149): loss=[17.50575766], w0=57.933663366336674, w1=2.0285064197514697\n",
      "Gradient Descent(85/149): loss=[17.07495743], w0=58.43267326732677, w1=2.2494370848672776\n",
      "Gradient Descent(86/149): loss=[16.6529673], w0=58.91089108910895, w1=2.4837982986028337\n",
      "Gradient Descent(87/149): loss=[16.24854073], w0=59.382178217821824, w1=2.7260245553531504\n",
      "Gradient Descent(88/149): loss=[15.84910521], w0=59.83960396039608, w1=2.978742333469136\n",
      "Gradient Descent(89/149): loss=[15.46691979], w0=60.262376237623805, w1=3.251528669355438\n",
      "Gradient Descent(90/149): loss=[15.10829462], w0=60.67821782178222, w1=3.5270865794242794\n",
      "Gradient Descent(91/149): loss=[14.75489635], w0=61.087128712871326, w1=3.806459183951815\n",
      "Gradient Descent(92/149): loss=[14.40452896], w0=61.49603960396043, w1=4.085831788479351\n",
      "Gradient Descent(93/149): loss=[14.05578703], w0=61.891089108910926, w1=4.373839384328607\n",
      "Gradient Descent(94/149): loss=[13.71462091], w0=62.27920792079211, w1=4.666037469532047\n",
      "Gradient Descent(95/149): loss=[13.38123631], w0=62.65346534653469, w1=4.959829093241769\n",
      "Gradient Descent(96/149): loss=[13.05882162], w0=63.02079207920796, w1=5.25705719205664\n",
      "Gradient Descent(97/149): loss=[12.74025172], w0=63.38118811881192, w1=5.560434316352406\n",
      "Gradient Descent(98/149): loss=[12.42321889], w0=63.74158415841588, w1=5.863811440648173\n",
      "Gradient Descent(99/149): loss=[12.10756173], w0=64.08811881188123, w1=6.172402175278548\n",
      "Gradient Descent(100/149): loss=[11.8006221], w0=64.42772277227726, w1=6.486369310516498\n",
      "Gradient Descent(101/149): loss=[11.49504179], w0=64.7673267326733, w1=6.800336445754448\n",
      "Gradient Descent(102/149): loss=[11.18946149], w0=65.10693069306933, w1=7.114303580992399\n",
      "Gradient Descent(103/149): loss=[10.88388119], w0=65.44653465346536, w1=7.428270716230349\n",
      "Gradient Descent(104/149): loss=[10.58459341], w0=65.76534653465349, w1=7.747893210218626\n",
      "Gradient Descent(105/149): loss=[10.29581653], w0=66.070297029703, w1=8.073669686866905\n",
      "Gradient Descent(106/149): loss=[10.01135208], w0=66.37524752475251, w1=8.399446163515185\n",
      "Gradient Descent(107/149): loss=[9.72808433], w0=66.6663366336634, w1=8.73297028041739\n",
      "Gradient Descent(108/149): loss=[9.44812546], w0=66.9574257425743, w1=9.066494397319596\n",
      "Gradient Descent(109/149): loss=[9.1710411], w0=67.23465346534658, w1=9.39863031947029\n",
      "Gradient Descent(110/149): loss=[8.90365613], w0=67.51188118811886, w1=9.730766241620982\n",
      "Gradient Descent(111/149): loss=[8.63627116], w0=67.78910891089114, w1=10.062902163771675\n",
      "Gradient Descent(112/149): loss=[8.37615192], w0=68.06633663366343, w1=10.363999289979422\n",
      "Gradient Descent(113/149): loss=[8.14054084], w0=68.32970297029709, w1=10.660466909273612\n",
      "Gradient Descent(114/149): loss=[7.9185445], w0=68.59306930693076, w1=10.943174379960814\n",
      "Gradient Descent(115/149): loss=[7.70527973], w0=68.85643564356442, w1=11.225881850648015\n",
      "Gradient Descent(116/149): loss=[7.49369583], w0=69.11287128712878, w1=11.504395843582206\n",
      "Gradient Descent(117/149): loss=[7.28999241], w0=69.35544554455453, w1=11.78820189306775\n",
      "Gradient Descent(118/149): loss=[7.09723404], w0=69.58415841584166, w1=12.060911465190971\n",
      "Gradient Descent(119/149): loss=[6.91990529], w0=69.80594059405948, w1=12.324245668386048\n",
      "Gradient Descent(120/149): loss=[6.75057353], w0=70.0277227722773, w1=12.587579871581125\n",
      "Gradient Descent(121/149): loss=[6.58474481], w0=70.25643564356443, w1=12.824765405096484\n",
      "Gradient Descent(122/149): loss=[6.43034328], w0=70.47821782178225, w1=13.065616959310148\n",
      "Gradient Descent(123/149): loss=[6.27807148], w0=70.69306930693077, w1=13.302953389983912\n",
      "Gradient Descent(124/149): loss=[6.13366333], w0=70.89405940594067, w1=13.525403099312918\n",
      "Gradient Descent(125/149): loss=[6.0058408], w0=71.08811881188126, w1=13.742945617944212\n",
      "Gradient Descent(126/149): loss=[5.88502183], w0=71.27524752475254, w1=13.953548196006844\n",
      "Gradient Descent(127/149): loss=[5.77163525], w0=71.46237623762383, w1=14.164150774069476\n",
      "Gradient Descent(128/149): loss=[5.66716206], w0=71.62178217821788, w1=14.349779559473173\n",
      "Gradient Descent(129/149): loss=[5.58672677], w0=71.75346534653471, w1=14.51689010761231\n",
      "Gradient Descent(130/149): loss=[5.52384781], w0=71.87128712871292, w1=14.670791185324186\n",
      "Gradient Descent(131/149): loss=[5.48009371], w0=71.95445544554461, w1=14.780276456654521\n",
      "Gradient Descent(132/149): loss=[5.453088], w0=72.0376237623763, w1=14.889761727984856\n",
      "Gradient Descent(133/149): loss=[5.42739263], w0=72.10693069306937, w1=14.985916181776727\n",
      "Gradient Descent(134/149): loss=[5.40732245], w0=72.17623762376245, w1=15.082070635568597\n",
      "Gradient Descent(135/149): loss=[5.38725226], w0=72.24554455445552, w1=15.178225089360467\n",
      "Gradient Descent(136/149): loss=[5.37046078], w0=72.30099009900998, w1=15.25972348971591\n",
      "Gradient Descent(137/149): loss=[5.35740652], w0=72.34950495049513, w1=15.335091856448138\n",
      "Gradient Descent(138/149): loss=[5.34592926], w0=72.39801980198028, w1=15.410460223180365\n",
      "Gradient Descent(139/149): loss=[5.33571466], w0=72.43267326732682, w1=15.469961786755725\n",
      "Gradient Descent(140/149): loss=[5.33004391], w0=72.46039603960405, w1=15.51864528583281\n",
      "Gradient Descent(141/149): loss=[5.32567643], w0=72.48811881188128, w1=15.561592159086487\n",
      "Gradient Descent(142/149): loss=[5.32217673], w0=72.5019801980199, w1=15.597828332032526\n",
      "Gradient Descent(143/149): loss=[5.32011131], w0=72.52277227722782, w1=15.624722856626713\n",
      "Gradient Descent(144/149): loss=[5.31847828], w0=72.55049504950505, w1=15.642690329098\n",
      "Gradient Descent(145/149): loss=[5.31724005], w0=72.56435643564366, w1=15.664356578291091\n",
      "Gradient Descent(146/149): loss=[5.31640655], w0=72.58514851485158, w1=15.677095775361284\n",
      "Gradient Descent(147/149): loss=[5.31555712], w0=72.6059405940595, w1=15.689834972431477\n",
      "Gradient Descent(148/149): loss=[5.3147077], w0=72.62673267326743, w1=15.70257416950167\n",
      "Gradient Descent(149/149): loss=[5.31387688], w0=72.64059405940604, w1=15.72424041869476\n",
      "Gradient Descent: execution time=0.055 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "sub_gradient_losses, sub_gradient_ws = sub_gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8788a2bcaf2e40fca4b310a000ff509d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=151, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sub_gradient_losses, sub_gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=[[5739.67022907]], w0=51.84746409844842, w1=7.7244264061924195\n",
      "Gradient Descent(1/149): loss=[[636.5642494]], w0=67.40170332798297, w1=10.041754328050114\n",
      "Gradient Descent(2/149): loss=[[177.28471123]], w0=72.06797509684336, w1=10.736952704607411\n",
      "Gradient Descent(3/149): loss=[[135.9495528]], w0=73.46785662750146, w1=10.945512217574597\n",
      "Gradient Descent(4/149): loss=[[132.22938854]], w0=73.88782108669889, w1=11.00808007146475\n",
      "Gradient Descent(5/149): loss=[[131.89457376]], w0=74.01381042445813, w1=11.026850427631798\n",
      "Gradient Descent(6/149): loss=[[131.86444042]], w0=74.0516072257859, w1=11.032481534481914\n",
      "Gradient Descent(7/149): loss=[[131.86172842]], w0=74.06294626618423, w1=11.034170866536945\n",
      "Gradient Descent(8/149): loss=[[131.86148434]], w0=74.06634797830372, w1=11.034677666153454\n",
      "Gradient Descent(9/149): loss=[[131.86146238]], w0=74.06736849193958, w1=11.034829706038408\n",
      "Gradient Descent(10/149): loss=[[131.8614604]], w0=74.06767464603033, w1=11.034875318003895\n",
      "Gradient Descent(11/149): loss=[[131.86146022]], w0=74.06776649225755, w1=11.034889001593541\n",
      "Gradient Descent(12/149): loss=[[131.86146021]], w0=74.06779404612573, w1=11.034893106670431\n",
      "Gradient Descent(13/149): loss=[[131.86146021]], w0=74.06780231228618, w1=11.034894338193501\n",
      "Gradient Descent(14/149): loss=[[131.86146021]], w0=74.06780479213431, w1=11.034894707650421\n",
      "Gradient Descent(15/149): loss=[[131.86146021]], w0=74.06780553608874, w1=11.034894818487496\n",
      "Gradient Descent(16/149): loss=[[131.86146021]], w0=74.06780575927507, w1=11.03489485173862\n",
      "Gradient Descent(17/149): loss=[[131.86146021]], w0=74.06780582623098, w1=11.034894861713957\n",
      "Gradient Descent(18/149): loss=[[131.86146021]], w0=74.06780584631775, w1=11.034894864706557\n",
      "Gradient Descent(19/149): loss=[[131.86146021]], w0=74.06780585234378, w1=11.034894865604338\n",
      "Gradient Descent(20/149): loss=[[131.86146021]], w0=74.06780585415159, w1=11.034894865873675\n",
      "Gradient Descent(21/149): loss=[[131.86146021]], w0=74.06780585469393, w1=11.034894865954474\n",
      "Gradient Descent(22/149): loss=[[131.86146021]], w0=74.06780585485663, w1=11.034894865978712\n",
      "Gradient Descent(23/149): loss=[[131.86146021]], w0=74.06780585490544, w1=11.034894865985985\n",
      "Gradient Descent(24/149): loss=[[131.86146021]], w0=74.0678058549201, w1=11.034894865988166\n",
      "Gradient Descent(25/149): loss=[[131.86146021]], w0=74.06780585492449, w1=11.034894865988822\n",
      "Gradient Descent(26/149): loss=[[131.86146021]], w0=74.06780585492581, w1=11.034894865989015\n",
      "Gradient Descent(27/149): loss=[[131.86146021]], w0=74.06780585492619, w1=11.034894865989076\n",
      "Gradient Descent(28/149): loss=[[131.86146021]], w0=74.06780585492632, w1=11.034894865989099\n",
      "Gradient Descent(29/149): loss=[[131.86146021]], w0=74.06780585492635, w1=11.0348948659891\n",
      "Gradient Descent(30/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(31/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(32/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(33/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(34/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(35/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(36/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(37/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(38/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(39/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(40/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(41/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(42/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(43/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(44/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(45/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(46/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(47/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(48/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(49/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(50/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(51/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(52/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(53/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(54/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(55/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(56/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(57/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(58/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(59/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(60/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(61/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(62/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(63/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(64/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(65/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(66/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(67/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(68/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(69/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(70/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(71/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(72/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(73/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(74/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(75/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(76/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(77/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(78/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(79/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(80/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(81/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(82/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(83/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(84/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(85/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(86/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(87/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(88/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(89/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(90/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(91/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(92/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(93/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(94/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(95/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(96/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(97/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(98/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(99/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(100/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(101/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(102/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(103/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(104/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(105/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(106/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(107/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(108/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(109/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(110/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(111/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(112/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(113/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(114/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(115/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(116/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(117/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(118/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(119/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(120/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(121/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(122/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(123/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(124/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(125/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(126/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(127/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(128/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(129/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(130/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(131/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(132/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(133/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(134/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(135/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(136/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(137/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(138/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(139/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(140/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(141/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(142/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(143/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(144/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(145/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(146/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(147/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(148/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(149/149): loss=[[131.86146021]], w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent(0/149): loss=[74.06780585], w0=0.7, w1=6.109524327590712e-16\n",
      "Gradient Descent(1/149): loss=[73.36780585], w0=1.4, w1=1.2219048655181425e-15\n",
      "Gradient Descent(2/149): loss=[72.66780585], w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "Gradient Descent(3/149): loss=[71.96780585], w0=2.8, w1=2.443809731036285e-15\n",
      "Gradient Descent(4/149): loss=[71.26780585], w0=3.5, w1=3.054762163795356e-15\n",
      "Gradient Descent(5/149): loss=[70.56780585], w0=4.2, w1=3.665714596554428e-15\n",
      "Gradient Descent(6/149): loss=[69.86780585], w0=4.9, w1=4.276667029313499e-15\n",
      "Gradient Descent(7/149): loss=[69.16780585], w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "Gradient Descent(8/149): loss=[68.46780585], w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "Gradient Descent(9/149): loss=[67.76780585], w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "Gradient Descent(10/149): loss=[67.06780585], w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "Gradient Descent(11/149): loss=[66.36780585], w0=8.4, w1=7.331429193108857e-15\n",
      "Gradient Descent(12/149): loss=[65.66780585], w0=9.1, w1=7.942381625867928e-15\n",
      "Gradient Descent(13/149): loss=[64.96780585], w0=9.799999999999999, w1=8.553334058627e-15\n",
      "Gradient Descent(14/149): loss=[64.26780585], w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "Gradient Descent(15/149): loss=[63.56780585], w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "Gradient Descent(16/149): loss=[62.86780585], w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "Gradient Descent(17/149): loss=[62.16780585], w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "Gradient Descent(18/149): loss=[61.46780585], w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "Gradient Descent(19/149): loss=[60.76780585], w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "Gradient Descent(20/149): loss=[60.06780585], w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "Gradient Descent(21/149): loss=[59.36780585], w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "Gradient Descent(22/149): loss=[58.66780585], w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "Gradient Descent(23/149): loss=[57.96780585], w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "Gradient Descent(24/149): loss=[57.26780585], w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "Gradient Descent(25/149): loss=[56.56780585], w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "Gradient Descent(26/149): loss=[55.86780585], w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "Gradient Descent(27/149): loss=[55.16780585], w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "Gradient Descent(28/149): loss=[54.46780585], w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "Gradient Descent(29/149): loss=[53.76780585], w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "Gradient Descent(30/149): loss=[53.06780585], w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "Gradient Descent(31/149): loss=[52.36780585], w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "Gradient Descent(32/149): loss=[51.66780585], w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "Gradient Descent(33/149): loss=[50.96780585], w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "Gradient Descent(34/149): loss=[50.26780585], w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "Gradient Descent(35/149): loss=[49.56780585], w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "Gradient Descent(36/149): loss=[48.86780585], w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "Gradient Descent(37/149): loss=[48.16780585], w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "Gradient Descent(38/149): loss=[47.46780585], w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "Gradient Descent(39/149): loss=[46.76780585], w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "Gradient Descent(40/149): loss=[46.06780585], w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "Gradient Descent(41/149): loss=[45.36780585], w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "Gradient Descent(42/149): loss=[44.66780585], w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "Gradient Descent(43/149): loss=[43.96780585], w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "Gradient Descent(44/149): loss=[43.26780585], w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "Gradient Descent(45/149): loss=[42.56780585], w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "Gradient Descent(46/149): loss=[41.86780585], w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "Gradient Descent(47/149): loss=[41.16780585], w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "Gradient Descent(48/149): loss=[40.46780585], w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "Gradient Descent(49/149): loss=[39.76780585], w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "Gradient Descent(50/149): loss=[39.06780585], w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "Gradient Descent(51/149): loss=[38.36780585], w0=36.4, w1=3.176952650347169e-14\n",
      "Gradient Descent(52/149): loss=[37.66780585], w0=37.1, w1=3.238047893623076e-14\n",
      "Gradient Descent(53/149): loss=[36.96780585], w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "Gradient Descent(54/149): loss=[36.26780585], w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "Gradient Descent(55/149): loss=[35.56780585], w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "Gradient Descent(56/149): loss=[34.86780585], w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "Gradient Descent(57/149): loss=[34.16780585], w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "Gradient Descent(58/149): loss=[33.46780585], w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "Gradient Descent(59/149): loss=[32.76780585], w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "Gradient Descent(60/149): loss=[32.06780585], w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "Gradient Descent(61/149): loss=[31.36780585], w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "Gradient Descent(62/149): loss=[30.66780585], w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "Gradient Descent(63/149): loss=[29.96780585], w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "Gradient Descent(64/149): loss=[29.26780585], w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "Gradient Descent(65/149): loss=[28.56780585], w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "Gradient Descent(66/149): loss=[27.86780585], w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "Gradient Descent(67/149): loss=[27.17327021], w0=47.59306930693074, w1=0.011147845678271063\n",
      "Gradient Descent(68/149): loss=[26.49045156], w0=48.279207920792125, w1=0.03308574108989941\n",
      "Gradient Descent(69/149): loss=[25.81721232], w0=48.96534653465351, w1=0.05502363650152776\n",
      "Gradient Descent(70/149): loss=[25.15503943], w0=49.63069306930698, w1=0.10538326388307814\n",
      "Gradient Descent(71/149): loss=[24.52410341], w0=50.28910891089114, w1=0.16746568532793435\n",
      "Gradient Descent(72/149): loss=[23.89929535], w0=50.947524752475296, w1=0.22954810677279056\n",
      "Gradient Descent(73/149): loss=[23.28439293], w0=51.59207920792084, w1=0.31242512932747524\n",
      "Gradient Descent(74/149): loss=[22.68687644], w0=52.22277227722777, w1=0.4119501328839991\n",
      "Gradient Descent(75/149): loss=[22.10626757], w0=52.84653465346539, w1=0.5208167847923756\n",
      "Gradient Descent(76/149): loss=[21.53781883], w0=53.4564356435644, w1=0.6457900912635992\n",
      "Gradient Descent(77/149): loss=[20.98633987], w0=54.0594059405941, w1=0.7796904498577214\n",
      "Gradient Descent(78/149): loss=[20.44556094], w0=54.655445544554496, w1=0.9197570104995693\n",
      "Gradient Descent(79/149): loss=[19.91191016], w0=55.24455445544559, w1=1.0670920297849913\n",
      "Gradient Descent(80/149): loss=[19.38964409], w0=55.819801980198065, w1=1.2261255948210765\n",
      "Gradient Descent(81/149): loss=[18.88798906], w0=56.36732673267331, w1=1.410709342622213\n",
      "Gradient Descent(82/149): loss=[18.4159605], w0=56.900990099009945, w1=1.605853732220269\n",
      "Gradient Descent(83/149): loss=[17.95489854], w0=57.42772277227727, w1=1.808762802293962\n",
      "Gradient Descent(84/149): loss=[17.50575766], w0=57.933663366336674, w1=2.0285064197514697\n",
      "Gradient Descent(85/149): loss=[17.07495743], w0=58.43267326732677, w1=2.2494370848672776\n",
      "Gradient Descent(86/149): loss=[16.6529673], w0=58.91089108910895, w1=2.4837982986028337\n",
      "Gradient Descent(87/149): loss=[16.24854073], w0=59.382178217821824, w1=2.7260245553531504\n",
      "Gradient Descent(88/149): loss=[15.84910521], w0=59.83960396039608, w1=2.978742333469136\n",
      "Gradient Descent(89/149): loss=[15.46691979], w0=60.262376237623805, w1=3.251528669355438\n",
      "Gradient Descent(90/149): loss=[15.10829462], w0=60.67821782178222, w1=3.5270865794242794\n",
      "Gradient Descent(91/149): loss=[14.75489635], w0=61.087128712871326, w1=3.806459183951815\n",
      "Gradient Descent(92/149): loss=[14.40452896], w0=61.49603960396043, w1=4.085831788479351\n",
      "Gradient Descent(93/149): loss=[14.05578703], w0=61.891089108910926, w1=4.373839384328607\n",
      "Gradient Descent(94/149): loss=[13.71462091], w0=62.27920792079211, w1=4.666037469532047\n",
      "Gradient Descent(95/149): loss=[13.38123631], w0=62.65346534653469, w1=4.959829093241769\n",
      "Gradient Descent(96/149): loss=[13.05882162], w0=63.02079207920796, w1=5.25705719205664\n",
      "Gradient Descent(97/149): loss=[12.74025172], w0=63.38118811881192, w1=5.560434316352406\n",
      "Gradient Descent(98/149): loss=[12.42321889], w0=63.74158415841588, w1=5.863811440648173\n",
      "Gradient Descent(99/149): loss=[12.10756173], w0=64.08811881188123, w1=6.172402175278548\n",
      "Gradient Descent(100/149): loss=[11.8006221], w0=64.42772277227726, w1=6.486369310516498\n",
      "Gradient Descent(101/149): loss=[11.49504179], w0=64.7673267326733, w1=6.800336445754448\n",
      "Gradient Descent(102/149): loss=[11.18946149], w0=65.10693069306933, w1=7.114303580992399\n",
      "Gradient Descent(103/149): loss=[10.88388119], w0=65.44653465346536, w1=7.428270716230349\n",
      "Gradient Descent(104/149): loss=[10.58459341], w0=65.76534653465349, w1=7.747893210218626\n",
      "Gradient Descent(105/149): loss=[10.29581653], w0=66.070297029703, w1=8.073669686866905\n",
      "Gradient Descent(106/149): loss=[10.01135208], w0=66.37524752475251, w1=8.399446163515185\n",
      "Gradient Descent(107/149): loss=[9.72808433], w0=66.6663366336634, w1=8.73297028041739\n",
      "Gradient Descent(108/149): loss=[9.44812546], w0=66.9574257425743, w1=9.066494397319596\n",
      "Gradient Descent(109/149): loss=[9.1710411], w0=67.23465346534658, w1=9.39863031947029\n",
      "Gradient Descent(110/149): loss=[8.90365613], w0=67.51188118811886, w1=9.730766241620982\n",
      "Gradient Descent(111/149): loss=[8.63627116], w0=67.78910891089114, w1=10.062902163771675\n",
      "Gradient Descent(112/149): loss=[8.37615192], w0=68.06633663366343, w1=10.363999289979422\n",
      "Gradient Descent(113/149): loss=[8.14054084], w0=68.32970297029709, w1=10.660466909273612\n",
      "Gradient Descent(114/149): loss=[7.9185445], w0=68.59306930693076, w1=10.943174379960814\n",
      "Gradient Descent(115/149): loss=[7.70527973], w0=68.85643564356442, w1=11.225881850648015\n",
      "Gradient Descent(116/149): loss=[7.49369583], w0=69.11287128712878, w1=11.504395843582206\n",
      "Gradient Descent(117/149): loss=[7.28999241], w0=69.35544554455453, w1=11.78820189306775\n",
      "Gradient Descent(118/149): loss=[7.09723404], w0=69.58415841584166, w1=12.060911465190971\n",
      "Gradient Descent(119/149): loss=[6.91990529], w0=69.80594059405948, w1=12.324245668386048\n",
      "Gradient Descent(120/149): loss=[6.75057353], w0=70.0277227722773, w1=12.587579871581125\n",
      "Gradient Descent(121/149): loss=[6.58474481], w0=70.25643564356443, w1=12.824765405096484\n",
      "Gradient Descent(122/149): loss=[6.43034328], w0=70.47821782178225, w1=13.065616959310148\n",
      "Gradient Descent(123/149): loss=[6.27807148], w0=70.69306930693077, w1=13.302953389983912\n",
      "Gradient Descent(124/149): loss=[6.13366333], w0=70.89405940594067, w1=13.525403099312918\n",
      "Gradient Descent(125/149): loss=[6.0058408], w0=71.08811881188126, w1=13.742945617944212\n",
      "Gradient Descent(126/149): loss=[5.88502183], w0=71.27524752475254, w1=13.953548196006844\n",
      "Gradient Descent(127/149): loss=[5.77163525], w0=71.46237623762383, w1=14.164150774069476\n",
      "Gradient Descent(128/149): loss=[5.66716206], w0=71.62178217821788, w1=14.349779559473173\n",
      "Gradient Descent(129/149): loss=[5.58672677], w0=71.75346534653471, w1=14.51689010761231\n",
      "Gradient Descent(130/149): loss=[5.52384781], w0=71.87128712871292, w1=14.670791185324186\n",
      "Gradient Descent(131/149): loss=[5.48009371], w0=71.95445544554461, w1=14.780276456654521\n",
      "Gradient Descent(132/149): loss=[5.453088], w0=72.0376237623763, w1=14.889761727984856\n",
      "Gradient Descent(133/149): loss=[5.42739263], w0=72.10693069306937, w1=14.985916181776727\n",
      "Gradient Descent(134/149): loss=[5.40732245], w0=72.17623762376245, w1=15.082070635568597\n",
      "Gradient Descent(135/149): loss=[5.38725226], w0=72.24554455445552, w1=15.178225089360467\n",
      "Gradient Descent(136/149): loss=[5.37046078], w0=72.30099009900998, w1=15.25972348971591\n",
      "Gradient Descent(137/149): loss=[5.35740652], w0=72.34950495049513, w1=15.335091856448138\n",
      "Gradient Descent(138/149): loss=[5.34592926], w0=72.39801980198028, w1=15.410460223180365\n",
      "Gradient Descent(139/149): loss=[5.33571466], w0=72.43267326732682, w1=15.469961786755725\n",
      "Gradient Descent(140/149): loss=[5.33004391], w0=72.46039603960405, w1=15.51864528583281\n",
      "Gradient Descent(141/149): loss=[5.32567643], w0=72.48811881188128, w1=15.561592159086487\n",
      "Gradient Descent(142/149): loss=[5.32217673], w0=72.5019801980199, w1=15.597828332032526\n",
      "Gradient Descent(143/149): loss=[5.32011131], w0=72.52277227722782, w1=15.624722856626713\n",
      "Gradient Descent(144/149): loss=[5.31847828], w0=72.55049504950505, w1=15.642690329098\n",
      "Gradient Descent(145/149): loss=[5.31724005], w0=72.56435643564366, w1=15.664356578291091\n",
      "Gradient Descent(146/149): loss=[5.31640655], w0=72.58514851485158, w1=15.677095775361284\n",
      "Gradient Descent(147/149): loss=[5.31555712], w0=72.6059405940595, w1=15.689834972431477\n",
      "Gradient Descent(148/149): loss=[5.3147077], w0=72.62673267326743, w1=15.70257416950167\n",
      "Gradient Descent(149/149): loss=[5.31387688], w0=72.64059405940604, w1=15.72424041869476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x113d6a470>]"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare gradient and sub_gradient decents\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.grid()\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "ax2.grid()\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.7\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "def prediction(w0, w1, mean_x, std_x):\n",
    "    \"\"\"Get the regression line from the model.\"\"\"\n",
    "    x = np.arange(1.2, 2, 0.01)\n",
    "    x_normalized = (x - mean_x) / std_x\n",
    "    return x, w0 + w1 * x_normalized\n",
    "\n",
    "# Start GD.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "wgd = gd_ws[-1]\n",
    "\n",
    "pred_x, pred_y = prediction(\n",
    "        wgd[0], wgd[1],\n",
    "        mean_x, std_x)\n",
    "\n",
    "ax1.plot(pred_x, pred_y, 'r')\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = sub_gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "wsgd = sgd_ws[-1]\n",
    "\n",
    "pred_x, pred_y = prediction(\n",
    "        wsgd[0], wsgd[1],\n",
    "        mean_x, std_x)\n",
    "\n",
    "ax2.plot(pred_x, pred_y, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Sub_Gradiant_Descent with MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_sub_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    \n",
    "    return compute_sub_gradient(y, tx, w)\n",
    "    \n",
    "\n",
    "\n",
    "def stochastic_sub_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "      \n",
    "    gen = batch_iter(y, tx, batch_size)\n",
    "    \n",
    "    y = []\n",
    "    tx = []\n",
    "    for pair in gen:\n",
    "        y.append(pair[0])\n",
    "        tx.append(pair[1])\n",
    "        \n",
    "    y = np.asarray(y).T.reshape(10)\n",
    "    tx = np.asarray(tx).reshape((10,2))\n",
    "    \n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        grad = compute_stoch_sub_gradient(y, tx, w)\n",
    "\n",
    "        w = w - (gamma * grad)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=[76.75295318], w0=0.7, w1=0.14090366446838562\n",
      "Gradient Descent(1/149): loss=[76.02459054], w0=1.4, w1=0.28180732893677124\n",
      "Gradient Descent(2/149): loss=[75.29622791], w0=2.0999999999999996, w1=0.42271099340515683\n",
      "Gradient Descent(3/149): loss=[74.56786528], w0=2.8, w1=0.5636146578735425\n",
      "Gradient Descent(4/149): loss=[73.83950265], w0=3.5, w1=0.7045183223419281\n",
      "Gradient Descent(5/149): loss=[73.11114001], w0=4.2, w1=0.8454219868103138\n",
      "Gradient Descent(6/149): loss=[72.38277738], w0=4.9, w1=0.9863256512786994\n",
      "Gradient Descent(7/149): loss=[71.65441475], w0=5.6000000000000005, w1=1.127229315747085\n",
      "Gradient Descent(8/149): loss=[70.92605212], w0=6.300000000000001, w1=1.2681329802154706\n",
      "Gradient Descent(9/149): loss=[70.19768949], w0=7.000000000000001, w1=1.4090366446838563\n",
      "Gradient Descent(10/149): loss=[69.46932685], w0=7.700000000000001, w1=1.549940309152242\n",
      "Gradient Descent(11/149): loss=[68.74096422], w0=8.4, w1=1.6908439736206275\n",
      "Gradient Descent(12/149): loss=[68.01260159], w0=9.1, w1=1.8317476380890132\n",
      "Gradient Descent(13/149): loss=[67.28423896], w0=9.799999999999999, w1=1.9726513025573988\n",
      "Gradient Descent(14/149): loss=[66.55587632], w0=10.499999999999998, w1=2.1135549670257845\n",
      "Gradient Descent(15/149): loss=[65.82751369], w0=11.199999999999998, w1=2.25445863149417\n",
      "Gradient Descent(16/149): loss=[65.09915106], w0=11.899999999999997, w1=2.3953622959625553\n",
      "Gradient Descent(17/149): loss=[64.37078843], w0=12.599999999999996, w1=2.5362659604309408\n",
      "Gradient Descent(18/149): loss=[63.64242579], w0=13.299999999999995, w1=2.677169624899326\n",
      "Gradient Descent(19/149): loss=[62.91406316], w0=13.999999999999995, w1=2.8180732893677116\n",
      "Gradient Descent(20/149): loss=[62.18570053], w0=14.699999999999994, w1=2.958976953836097\n",
      "Gradient Descent(21/149): loss=[61.4573379], w0=15.399999999999993, w1=3.0998806183044825\n",
      "Gradient Descent(22/149): loss=[60.72897526], w0=16.099999999999994, w1=3.240784282772868\n",
      "Gradient Descent(23/149): loss=[60.00061263], w0=16.799999999999994, w1=3.3816879472412533\n",
      "Gradient Descent(24/149): loss=[59.27225], w0=17.499999999999993, w1=3.5225916117096387\n",
      "Gradient Descent(25/149): loss=[58.54388737], w0=18.199999999999992, w1=3.663495276178024\n",
      "Gradient Descent(26/149): loss=[57.81552473], w0=18.89999999999999, w1=3.8043989406464096\n",
      "Gradient Descent(27/149): loss=[57.0871621], w0=19.59999999999999, w1=3.945302605114795\n",
      "Gradient Descent(28/149): loss=[56.35879947], w0=20.29999999999999, w1=4.0862062695831805\n",
      "Gradient Descent(29/149): loss=[55.63043684], w0=20.99999999999999, w1=4.227109934051566\n",
      "Gradient Descent(30/149): loss=[54.90207421], w0=21.69999999999999, w1=4.368013598519952\n",
      "Gradient Descent(31/149): loss=[54.17371157], w0=22.399999999999988, w1=4.508917262988338\n",
      "Gradient Descent(32/149): loss=[53.44534894], w0=23.099999999999987, w1=4.649820927456724\n",
      "Gradient Descent(33/149): loss=[52.71698631], w0=23.799999999999986, w1=4.79072459192511\n",
      "Gradient Descent(34/149): loss=[51.98862368], w0=24.499999999999986, w1=4.931628256393496\n",
      "Gradient Descent(35/149): loss=[51.26026104], w0=25.199999999999985, w1=5.0725319208618815\n",
      "Gradient Descent(36/149): loss=[50.53189841], w0=25.899999999999984, w1=5.213435585330267\n",
      "Gradient Descent(37/149): loss=[49.80353578], w0=26.599999999999984, w1=5.354339249798653\n",
      "Gradient Descent(38/149): loss=[49.07517315], w0=27.299999999999983, w1=5.495242914267039\n",
      "Gradient Descent(39/149): loss=[48.34681051], w0=27.999999999999982, w1=5.636146578735425\n",
      "Gradient Descent(40/149): loss=[47.61844788], w0=28.69999999999998, w1=5.777050243203811\n",
      "Gradient Descent(41/149): loss=[46.89008525], w0=29.39999999999998, w1=5.917953907672197\n",
      "Gradient Descent(42/149): loss=[46.16172262], w0=30.09999999999998, w1=6.058857572140583\n",
      "Gradient Descent(43/149): loss=[45.43335998], w0=30.79999999999998, w1=6.1997612366089685\n",
      "Gradient Descent(44/149): loss=[44.70499735], w0=31.49999999999998, w1=6.340664901077354\n",
      "Gradient Descent(45/149): loss=[43.97663472], w0=32.19999999999998, w1=6.48156856554574\n",
      "Gradient Descent(46/149): loss=[43.24827209], w0=32.899999999999984, w1=6.622472230014126\n",
      "Gradient Descent(47/149): loss=[42.51990946], w0=33.59999999999999, w1=6.763375894482512\n",
      "Gradient Descent(48/149): loss=[41.79154682], w0=34.29999999999999, w1=6.904279558950898\n",
      "Gradient Descent(49/149): loss=[41.06318419], w0=34.99999999999999, w1=7.045183223419284\n",
      "Gradient Descent(50/149): loss=[40.33482156], w0=35.699999999999996, w1=7.18608688788767\n",
      "Gradient Descent(51/149): loss=[39.60645893], w0=36.4, w1=7.3269905523560555\n",
      "Gradient Descent(52/149): loss=[38.87809629], w0=37.1, w1=7.467894216824441\n",
      "Gradient Descent(53/149): loss=[38.14973366], w0=37.800000000000004, w1=7.608797881292827\n",
      "Gradient Descent(54/149): loss=[37.42137103], w0=38.50000000000001, w1=7.749701545761213\n",
      "Gradient Descent(55/149): loss=[36.6930084], w0=39.20000000000001, w1=7.890605210229599\n",
      "Gradient Descent(56/149): loss=[35.96464576], w0=39.90000000000001, w1=8.031508874697984\n",
      "Gradient Descent(57/149): loss=[35.23628313], w0=40.600000000000016, w1=8.17241253916637\n",
      "Gradient Descent(58/149): loss=[34.5079205], w0=41.30000000000002, w1=8.313316203634756\n",
      "Gradient Descent(59/149): loss=[33.77955787], w0=42.00000000000002, w1=8.454219868103142\n",
      "Gradient Descent(60/149): loss=[33.05119523], w0=42.700000000000024, w1=8.595123532571527\n",
      "Gradient Descent(61/149): loss=[32.3228326], w0=43.40000000000003, w1=8.736027197039913\n",
      "Gradient Descent(62/149): loss=[31.59446997], w0=44.10000000000003, w1=8.8769308615083\n",
      "Gradient Descent(63/149): loss=[30.86610734], w0=44.80000000000003, w1=9.017834525976685\n",
      "Gradient Descent(64/149): loss=[30.1377447], w0=45.500000000000036, w1=9.15873819044507\n",
      "Gradient Descent(65/149): loss=[29.40938207], w0=46.20000000000004, w1=9.299641854913457\n",
      "Gradient Descent(66/149): loss=[28.68101944], w0=46.90000000000004, w1=9.440545519381843\n",
      "Gradient Descent(67/149): loss=[27.95265681], w0=47.600000000000044, w1=9.581449183850228\n",
      "Gradient Descent(68/149): loss=[27.22429418], w0=48.30000000000005, w1=9.722352848318614\n",
      "Gradient Descent(69/149): loss=[26.49593154], w0=49.00000000000005, w1=9.863256512787\n",
      "Gradient Descent(70/149): loss=[25.76756891], w0=49.70000000000005, w1=10.004160177255386\n",
      "Gradient Descent(71/149): loss=[25.03920628], w0=50.400000000000055, w1=10.145063841723772\n",
      "Gradient Descent(72/149): loss=[24.31084365], w0=51.10000000000006, w1=10.285967506192158\n",
      "Gradient Descent(73/149): loss=[23.58248101], w0=51.80000000000006, w1=10.426871170660544\n",
      "Gradient Descent(74/149): loss=[22.85411838], w0=52.500000000000064, w1=10.56777483512893\n",
      "Gradient Descent(75/149): loss=[22.12575575], w0=53.20000000000007, w1=10.708678499597315\n",
      "Gradient Descent(76/149): loss=[21.39739312], w0=53.90000000000007, w1=10.849582164065701\n",
      "Gradient Descent(77/149): loss=[20.66903048], w0=54.60000000000007, w1=10.990485828534087\n",
      "Gradient Descent(78/149): loss=[19.94066785], w0=55.300000000000075, w1=11.131389493002473\n",
      "Gradient Descent(79/149): loss=[19.21230522], w0=56.00000000000008, w1=11.272293157470859\n",
      "Gradient Descent(80/149): loss=[18.48394259], w0=56.70000000000008, w1=11.413196821939245\n",
      "Gradient Descent(81/149): loss=[17.75557995], w0=57.400000000000084, w1=11.55410048640763\n",
      "Gradient Descent(82/149): loss=[17.02721732], w0=58.10000000000009, w1=11.695004150876017\n",
      "Gradient Descent(83/149): loss=[16.29885469], w0=58.80000000000009, w1=11.835907815344402\n",
      "Gradient Descent(84/149): loss=[15.57049206], w0=59.50000000000009, w1=11.976811479812788\n",
      "Gradient Descent(85/149): loss=[14.84212943], w0=60.200000000000095, w1=12.117715144281174\n",
      "Gradient Descent(86/149): loss=[14.11376679], w0=60.9000000000001, w1=12.25861880874956\n",
      "Gradient Descent(87/149): loss=[13.38540416], w0=61.6000000000001, w1=12.399522473217946\n",
      "Gradient Descent(88/149): loss=[12.65704153], w0=62.300000000000104, w1=12.540426137686332\n",
      "Gradient Descent(89/149): loss=[11.9286789], w0=63.00000000000011, w1=12.681329802154718\n",
      "Gradient Descent(90/149): loss=[11.26627693], w0=63.56000000000011, w1=12.908591982279788\n",
      "Gradient Descent(91/149): loss=[10.74449393], w0=64.1200000000001, w1=13.135854162404858\n",
      "Gradient Descent(92/149): loss=[10.22271093], w0=64.6800000000001, w1=13.363116342529928\n",
      "Gradient Descent(93/149): loss=[9.70092793], w0=65.24000000000011, w1=13.590378522654998\n",
      "Gradient Descent(94/149): loss=[9.17914493], w0=65.80000000000011, w1=13.817640702780068\n",
      "Gradient Descent(95/149): loss=[8.65736194], w0=66.36000000000011, w1=14.044902882905138\n",
      "Gradient Descent(96/149): loss=[8.13557894], w0=66.92000000000012, w1=14.272165063030208\n",
      "Gradient Descent(97/149): loss=[7.61379594], w0=67.48000000000012, w1=14.499427243155278\n",
      "Gradient Descent(98/149): loss=[7.14560545], w0=67.90000000000012, w1=14.691018999004385\n",
      "Gradient Descent(99/149): loss=[6.84116631], w0=68.32000000000012, w1=14.882610754853491\n",
      "Gradient Descent(100/149): loss=[6.53672717], w0=68.74000000000012, w1=15.074202510702598\n",
      "Gradient Descent(101/149): loss=[6.23228802], w0=69.16000000000012, w1=15.265794266551705\n",
      "Gradient Descent(102/149): loss=[5.92784888], w0=69.58000000000013, w1=15.457386022400811\n",
      "Gradient Descent(103/149): loss=[5.72587786], w0=69.86000000000013, w1=15.437697531974141\n",
      "Gradient Descent(104/149): loss=[5.6133241], w0=70.14000000000013, w1=15.41800904154747\n",
      "Gradient Descent(105/149): loss=[5.50077033], w0=70.42000000000013, w1=15.3983205511208\n",
      "Gradient Descent(106/149): loss=[5.38821656], w0=70.70000000000013, w1=15.37863206069413\n",
      "Gradient Descent(107/149): loss=[5.2756628], w0=70.98000000000013, w1=15.35894357026746\n",
      "Gradient Descent(108/149): loss=[5.16310903], w0=71.26000000000013, w1=15.33925507984079\n",
      "Gradient Descent(109/149): loss=[5.05055526], w0=71.54000000000013, w1=15.31956658941412\n",
      "Gradient Descent(110/149): loss=[4.9380015], w0=71.82000000000014, w1=15.299878098987449\n",
      "Gradient Descent(111/149): loss=[4.82544773], w0=72.10000000000014, w1=15.280189608560779\n",
      "Gradient Descent(112/149): loss=[4.71289396], w0=72.38000000000014, w1=15.260501118134108\n",
      "Gradient Descent(113/149): loss=[4.6003402], w0=72.66000000000014, w1=15.240812627707438\n",
      "Gradient Descent(114/149): loss=[4.48778643], w0=72.94000000000014, w1=15.221124137280768\n",
      "Gradient Descent(115/149): loss=[4.37523266], w0=73.22000000000014, w1=15.201435646854097\n",
      "Gradient Descent(116/149): loss=[4.29344606], w0=73.36000000000014, w1=15.341713303239969\n",
      "Gradient Descent(117/149): loss=[4.23733488], w0=73.50000000000014, w1=15.48199095962584\n",
      "Gradient Descent(118/149): loss=[4.18122371], w0=73.64000000000014, w1=15.622268616011711\n",
      "Gradient Descent(119/149): loss=[4.12511254], w0=73.78000000000014, w1=15.762546272397582\n",
      "Gradient Descent(120/149): loss=[4.06900137], w0=73.92000000000014, w1=15.902823928783453\n",
      "Gradient Descent(121/149): loss=[4.01289019], w0=74.06000000000014, w1=16.043101585169325\n",
      "Gradient Descent(122/149): loss=[3.95677902], w0=74.20000000000014, w1=16.183379241555194\n",
      "Gradient Descent(123/149): loss=[3.90066785], w0=74.34000000000015, w1=16.323656897941063\n",
      "Gradient Descent(124/149): loss=[3.84624296], w0=74.62000000000015, w1=16.303968407514393\n",
      "Gradient Descent(125/149): loss=[3.79250218], w0=74.76000000000015, w1=16.444246063900263\n",
      "Gradient Descent(126/149): loss=[3.73639101], w0=74.90000000000015, w1=16.584523720286132\n",
      "Gradient Descent(127/149): loss=[3.69491539], w0=74.76000000000015, w1=16.675566854574686\n",
      "Gradient Descent(128/149): loss=[3.6900351], w0=74.90000000000015, w1=16.815844510960556\n",
      "Gradient Descent(129/149): loss=[3.66699194], w0=74.90000000000015, w1=16.74692149843657\n",
      "Gradient Descent(130/149): loss=[3.67379367], w0=74.76000000000015, w1=16.837964632725125\n",
      "Gradient Descent(131/149): loss=[3.69270859], w0=75.04000000000015, w1=16.818276142298455\n",
      "Gradient Descent(132/149): loss=[3.69251317], w0=74.90000000000015, w1=16.90931927658701\n",
      "Gradient Descent(133/149): loss=[3.6761956], w0=74.90000000000015, w1=16.840396264063024\n",
      "Gradient Descent(134/149): loss=[3.66940934], w0=74.90000000000015, w1=16.771473251539035\n",
      "Gradient Descent(135/149): loss=[3.67060043], w0=74.76000000000015, w1=16.86251638582759\n",
      "Gradient Descent(136/149): loss=[3.69339914], w0=75.04000000000015, w1=16.84282789540092\n",
      "Gradient Descent(137/149): loss=[3.68931993], w0=74.90000000000015, w1=16.933871029689474\n",
      "Gradient Descent(138/149): loss=[3.678613], w0=74.90000000000015, w1=16.86494801716549\n",
      "Gradient Descent(139/149): loss=[3.67182674], w0=74.90000000000015, w1=16.7960250046415\n",
      "Gradient Descent(140/149): loss=[3.66740719], w0=74.76000000000015, w1=16.887068138930054\n",
      "Gradient Descent(141/149): loss=[3.6940897], w0=75.04000000000015, w1=16.867379648503384\n",
      "Gradient Descent(142/149): loss=[3.68612669], w0=74.90000000000015, w1=16.95842278279194\n",
      "Gradient Descent(143/149): loss=[3.6810304], w0=74.90000000000015, w1=16.889499770267953\n",
      "Gradient Descent(144/149): loss=[3.67424415], w0=74.90000000000015, w1=16.820576757743964\n",
      "Gradient Descent(145/149): loss=[3.66745789], w0=74.90000000000015, w1=16.751653745219976\n",
      "Gradient Descent(146/149): loss=[3.67317819], w0=74.76000000000015, w1=16.84269687950853\n",
      "Gradient Descent(147/149): loss=[3.69284169], w0=75.04000000000015, w1=16.82300838908186\n",
      "Gradient Descent(148/149): loss=[3.69189769], w0=74.90000000000015, w1=16.914051523370414\n",
      "Gradient Descent(149/149): loss=[3.67666155], w0=74.90000000000015, w1=16.84512851084643\n",
      "SGD: execution time=0.039 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.7\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "ssgd_losses, ssgd_ws = stochastic_sub_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f60b2e132d24baba16c23d411219324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=151, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        ssgd_losses, ssgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
